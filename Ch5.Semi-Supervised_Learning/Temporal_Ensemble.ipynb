{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(root, transform):\n",
    "    # Load Train Data\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # Load Test Data\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_dataset, test_dataset, batch_size, k, n_classes, seed, shuffle_train=False, return_idx=True):\n",
    "    # Randomly form unlabeled data in training dataset\n",
    "    n = len(train_dataset)  # Dataset size\n",
    "    rand_seed = np.random.RandomState(seed) # Set seed \n",
    "    indices = torch.zeros(k)  # Empty tensor for saving indices for keeping labeled data\n",
    "    unlabel_indices = torch.zeros(n - k)  # Empty tensor for indices of unlabeled data\n",
    "    quot = k // n_classes \n",
    "    temp_index = 0\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        class_items = (train_dataset.train_labels == i).nonzero()  # indices of samples with label i\n",
    "        # train_dataset.train_labels == i : Train Data 중 Label이 i인 것들만 True\n",
    "        # .nonzero(): Element 값이 0이 아닌 Element들의 Indices만 반환 (Element 값이 0이면 .nonzero() 결과에도 반환되지 않음)\n",
    "\n",
    "        n_class = len(class_items)  # number of samples with label i\n",
    "        shuffled = rand_seed.permutation(np.arange(n_class))  # shuffle them  |shuffled| = |n_class|\n",
    "        indices[i * quot: (i+1) * quot] = torch.squeeze(class_items[shuffled[:quot]]) # |class_items[shuffled[:quot]]| = (alpha, 1)이라서 2차원이라 Squeeze 적용\n",
    "        unlabel_indices[temp_index: temp_index+n_class-quot] = torch.squeeze(class_items[shuffled[quot:]])\n",
    "        temp_index += (n_class-quot)\n",
    "\n",
    "    unlabel_indices = unlabel_indices.long() # tensor as indices must be long, byte or bool\n",
    "    train_dataset.train_labels[unlabel_indices] = -1 # Unsupervised의 경우 Label = -1 할당\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=0,\n",
    "                                               shuffle=shuffle_train)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=0,\n",
    "                                              shuffle=False)\n",
    "\n",
    "    if return_idx:\n",
    "        return train_loader, test_loader, indices\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, batch_size, input_shape, std):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.shape = (batch_size,) + input_shape # |self.shape| = (batch_size, input_shape's first element, input_shape's second element, input_shape's third element)\n",
    "        self.std = std\n",
    "        self.noise = torch.zeros(self.shape).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.noise.normal_(mean=0, std=self.std) # torch.normal(mean, std): Returns a tensor of random numbers from separate normal distributions\n",
    "        return x + self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled Data에 대해서 (Supervised Loss) Cross Entropy 사용\n",
    "def labeled_ce_loss(out, labels):\n",
    "    cond = (labels >= 0) # 참고: preprocess_data()에서 Unlabeled Data의 Label = -1 할당함 \n",
    "    labeled_arr = torch.nonzero(cond) # Array of Labeled Sample Index\n",
    "    num_sup = len(labeled_arr) # Num of Supervised Samples\n",
    "    \n",
    "    # Supervised Instance 수가 0보다 많다면, 즉 존재한다면\n",
    "    if num_sup > 0:\n",
    "        labeled_outputs = torch.index_select(input=out, dim=0, index=labeled_arr.view(num_sup)) # labeled_arr.view(num_sup): Flatten() 역할\n",
    "        labeled_labels = labels[cond] \n",
    "        loss = F.cross_entropy(labeled_outputs, labeled_labels)\n",
    "        return loss, num_sup\n",
    "    \n",
    "    # Supervised Instance가 없다면 CE Loss = 0\n",
    "    loss = torch.tensor([0.], requires_grad=False).cuda()\n",
    "    return loss, 0 # num_sup == 0 이면 loss와 0(num_sup이 없다는 뜻) 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Loss로 MSE Loss 사용 \n",
    "def mse_loss(cur_out, ensem_out):\n",
    "    # Current Output과 Ensemble Output 간의 MSE\n",
    "    se = torch.sum((F.softmax(cur_out, dim=1) - F.softmax(ensem_out, dim=1)) ** 2)\n",
    "    \n",
    "    return se/len(cur_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_losses(cur_out, ensem_out, w, labels):\n",
    "    # cur_out: Current output\n",
    "    # ensem_out: Ensemble output\n",
    "    # w: Weight for summation loss\n",
    "\n",
    "    sup_loss, nbsup = labeled_ce_loss(cur_out, labels)\n",
    "    unsup_loss = mse_loss(cur_out, ensem_out)\n",
    "    total_loss = sup_loss + w * unsup_loss # 최종 Loss = Supervised Loss + w*Unsupervised Loss\n",
    "\n",
    "    return total_loss, sup_loss, unsup_loss, nbsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Loss와 Unsupervised Loss를 더할 때 Unsupervised Loss Term 앞에 붙는 Weight를 조정하는 함수\n",
    "def weight_ramp_up(epoch:int, max_epochs:int, max_val:float, mult, n_labeled:int, n_samples:int):\n",
    "\n",
    "    max_val = max_val * (n_labeled/n_samples)\n",
    "\n",
    "    if epoch == 0:\n",
    "        return 0.\n",
    "    elif epoch >= max_epochs:\n",
    "        return max_val\n",
    "\n",
    "    return max_val * np.exp(-mult * (1. - float(epoch)/max_epochs)**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model로 CNN 사용\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, batch_size:int, std:float, input_shape:tuple = (1,28,28), drop_out:float = 0.5, first_layer:int = 16, second_layer:int = 32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.std = std\n",
    "        self.drop_out = drop_out\n",
    "        self.first_layer = first_layer\n",
    "        self.second_layer = second_layer\n",
    "        self.input_shape = input_shape\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(1, self.first_layer, 3, stride=1, padding=1),\n",
    "                                        nn.BatchNorm2d(self.first_layer),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2, padding=1))\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(self.first_layer, self.second_layer, 3, stride=1, padding=1),\n",
    "                                        nn.BatchNorm2d(self.second_layer),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2, padding=1))\n",
    "        self.drop = nn.Dropout(self.drop_out)\n",
    "        self.fc = nn.Linear(self.second_layer * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            b = x.size(0)\n",
    "            noise = GaussianNoise(b, self.input_shape, self.std)\n",
    "            x = noise(x)\n",
    "\n",
    "        # first block\n",
    "        x = self.conv_block1(x)\n",
    "\n",
    "        # second block\n",
    "        x = self.conv_block2(x)\n",
    "\n",
    "        # Classifier (FC)\n",
    "        x = x.view(-1, self.second_layer * 7 * 7) # Flatten\n",
    "        x = self.fc(self.drop(x)) # Apply Dropout\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        k:int, \n",
    "        alpha:float, \n",
    "        lr:float, \n",
    "        num_epochs:int, \n",
    "        batch_size:int, \n",
    "        n_instances:int, \n",
    "        n_classes:int = 10, \n",
    "        max_epochs:int = 80, \n",
    "        max_val:float = 1.\n",
    "        ):    \n",
    "    \n",
    "    # Feed model to GPU if available\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    # Setting Optimizer Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Set First Ensemble Output as zeros\n",
    "    Z = torch.zeros(n_instances, n_classes).float().to(device)\n",
    "    z = torch.zeros(n_instances, n_classes).float().to(device)\n",
    "    outputs = torch.zeros(n_instances, n_classes).float().to(device)\n",
    "\n",
    "    losses = [] # Total Loss\n",
    "    sup_losses = [] # Supervised Loss (Cross-Entropy Loss)\n",
    "    unsup_losses = [] # Unsupervised Loss (MSE Loss)\n",
    "    best_loss = 10_000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Calculate Unsupervised Loss Weight\n",
    "        w = weight_ramp_up(epoch, max_epochs, max_val, 5, k, 60000)\n",
    "        w = torch.tensor(w, requires_grad=False).to(device)\n",
    "\n",
    "        # Targets change only once per Epoch\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            batch_size = images.size(0)  # retrieve batch size again cause drop last is false\n",
    "            images = images.to(device)\n",
    "            labels = labels.requires_grad_(False).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(images)\n",
    "\n",
    "            # 현재 Batch에 해당하는 Ensemble 결과 가져오기\n",
    "            z_ = z[i*batch_size: (i+1)*batch_size]\n",
    "            z_.requires_grad_(False)\n",
    "            loss, sup_loss, unsup_loss, nbsup = return_losses(out, z_, w, labels)\n",
    "\n",
    "            # Save outputs\n",
    "            outputs[i*batch_size: (i+1)*batch_size] = out.detach().clone()\n",
    "            losses.append(loss.item())\n",
    "            sup_losses.append(nbsup*sup_loss.item())\n",
    "            unsup_losses.append(unsup_loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_mean = np.mean(losses)\n",
    "        sup_loss_mean = np.mean(sup_losses)\n",
    "        unsup_loss_mean = np.mean(unsup_losses)\n",
    "\n",
    "        # 5 Epoch마다 Total Loss, Supervised Loss, Unsupervised Loss 출력\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_mean:.4f}, Supervised Loss: {sup_loss_mean:.4f}, Unsupervised Loss: {unsup_loss_mean:.4f}')\n",
    "    \n",
    "        # Model의 Outputs에 EMA 이용해서 Ensemble Outputs으로 Update\n",
    "        Z = alpha * Z + (1. - alpha) * outputs\n",
    "        z = Z * (1. / (1. - alpha ** (epoch + 1)))\n",
    "\n",
    "        if loss_mean < best_loss:\n",
    "            best_loss = loss_mean\n",
    "            \n",
    "            print('='*10, f'{epoch + 1} Epoch Model is Saved', '='*10)\n",
    "            torch.save({'state_dict': model.state_dict()}, f'model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader):\n",
    "    # Evaluation using Best Model\n",
    "    checkpoint = torch.load('model_best.pth')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (samples, labels) in enumerate(loader):\n",
    "        samples = samples.cuda()\n",
    "        labels = labels.requires_grad_(False).cuda()\n",
    "        outputs = model(samples)\n",
    "        _, predicted = torch.max(outputs.detach(), 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.detach().view_as(predicted)).sum()\n",
    "    \n",
    "    accuracy = 100 * float(correct) / total\n",
    "    \n",
    "    print(\"=\"*10, \"Evaluation Result\", \"=\"*10)\n",
    "    print(f'Evaluation Result - Accuracy: {accuracy:.2f}')\n",
    "    return np.round(accuracy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normalizing dataset(MNIST)\n",
    "m = 0.13 # Decided by manually checked MNIST OG Dataset\n",
    "s = 0.31 # Decided by manually checked MNIST OG Dataset \n",
    "\n",
    "# Model\n",
    "drop = 0.3 # dropout probability\n",
    "std = 0.15 # std of gaussian noise\n",
    "first_layer = 16 # channels of the first conv\n",
    "second_layer = 32 # channels of the second conv\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.002\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "\n",
    "# Temporal ensembling vars\n",
    "alpha = 0.5 # Alpha which is in EMA (Output Ensemble)\n",
    "\n",
    "# keep k labeled data in training set, others' labels will be deleted to make artificial Unlabeled data \n",
    "k = 500 # if k = 500, then within Supervised Dataset there are (500/num of classes) instances for each class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(m, s)])\n",
    "train_dataset, val_dataset = mnist_dataset(root='~/datasets/MNIST', transform=transform)\n",
    "ntrain = len(train_dataset)\n",
    "\n",
    "model = CNN(batch_size, std, first_layer=first_layer, second_layer=second_layer)\n",
    "train_loader, val_loader, indices = preprocess_data(train_dataset, val_dataset, batch_size=batch_size, k=k, n_classes=10, seed=1002, shuffle_train=False)\n",
    "train(model, train_loader, val_loader, k, alpha, learning_rate, num_epochs, batch_size, ntrain)\n",
    "_ = evaluation(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Change alpha\n",
    "alphas = np.arange(0,1+0.1,0.1) # [0., 0.1, ..., 1.]\n",
    "alpha_exp_accs = []\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(m, s)])\n",
    "train_dataset, val_dataset = mnist_dataset(root='~/datasets/MNIST', transform=transform)\n",
    "ntrain = len(train_dataset)\n",
    "\n",
    "for a in alphas:\n",
    "    model = CNN(batch_size, std, first_layer=first_layer, second_layer=second_layer)\n",
    "    train_loader, val_loader, indices = preprocess_data(train_dataset, val_dataset, batch_size=batch_size, k=k, n_classes=10, seed=1002, shuffle_train=False)\n",
    "    train(model, train_loader, val_loader, k, a, learning_rate, num_epochs, batch_size, ntrain)\n",
    "    alpha_exp_accs.append(evaluation(model, val_loader))\n",
    "\n",
    "print(alpha_exp_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
