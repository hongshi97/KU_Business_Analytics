# **Semi-Supervised Learning**
- ë³¸ Tutorialì€ ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ëŒ€í•™ì› Business Analytics ê°•ì˜ ìë£Œ ë° ì™¸ë¶€ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
- ë³¸ Tutorialì—ì„œëŠ” Semi-Supervised Learning ë°©ë²•ë¡  ì¤‘ Temporal Ensemble êµ¬í˜„ì— ê´€í•œ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

---
## Overview
ğŸ¤” What is Semi-supervised Learning?
<p align = 'center'>
<img src = 'https://user-images.githubusercontent.com/56019094/209630453-8fa04984-26b1-4d7b-8a1e-8d125c61e263.png' width = 85%>
</p>

<center>
ì´ë¯¸ì§€ ì¶œì²˜: https://blog.est.ai/2020/11/ssl/
</center>

**Supervised Learning**ì€ Labeled Dataë¥¼ ì´ìš©í•˜ê³ , **Unsupervised Learning**ì€ Label ì •ë³´ê°€ ì—†ëŠ” ì¦‰, Unlabeled Dataë¥¼ ì´ìš©í•œë‹¤ëŠ” ê²ƒì€ ë‹¤ë“¤ ì•Œê³  ê³„ì‹¤ ê²ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ```Semi-supervised Learning```ì€ ë¬´ì—‡ì„ ì´ìš©í• ê¹Œìš”?  

ìœ„ Figureë¥¼ ë³´ë©´ Semi-supervised Learningì€ ëŒ€ë¶€ë¶„ì´ Unlabeled Data(ìƒ‰ì´ ì—†ëŠ” ì›)ê³ , ì¡°ê¸ˆì˜ Labeled Data(ìƒ‰ì´ ìˆëŠ” ì›)ê°€ ìˆìŠµë‹ˆë‹¤.   

>semi-  
: 'ë°˜', 'ì–´ëŠ ì •ë„ì˜'ì˜ ëœ»ì„ ë‚˜íƒ€ëƒ„  

ìœ„ Figureì™€ ```semi-```ì˜ ì‚¬ì „ì  ì˜ë¯¸ë¥¼ í†µí•´ **Semi**-supervised Learningì´ë¼ëŠ” í•™ìŠµ ë°©ë²•ì€ **'ì–´ëŠ ì •ë„'** Supervised Learningì…ë‹ˆë‹¤. ì¦‰, Semi-supervised Learningì—ëŠ” Labeled Dataì™€ Unlabeled Dataê°€ ë‘˜ ë‹¤ ì‚¬ìš©ë©ë‹ˆë‹¤.  

â¢ ì†ŒëŸ‰ì˜ Labeled Dataì—ëŠ” Supervised Learningì„, ëŒ€ëŸ‰ì˜ Unlabeled Dataì—ëŠ” Unsupervised Learningì„ ì ìš©í•˜ì—¬ ì „ì²´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤.
  
ğŸ’° Why?  

Label ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” "Labeling" ì‘ì—…ì— ì†Œìš”ë˜ëŠ” ë§ì€ ìì›ê³¼ ë¹„ìš© ë•Œë¬¸ì— Semi-supervised Learning ë°©ë²•ë¡ ì´ ë“±ì¥í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. 

<hr/>

## Temporal Ensemble

ë³¸ Tutorialì—ì„œ ë¦¬ë·°í•  Temporal Ensembleì€ Semi-supervised Learning ë°©ë²•ë¡  ì¤‘ ```Consistency Regularization```ì— í•´ë‹¹í•©ë‹ˆë‹¤. 

- Consistency Regularizationë€?  
    - Unlabeled Dataì— Realistic Perturbationì„ ì ìš©í•´ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì¼ê´€ì ì¼ ê²ƒì´ë¼ëŠ” ê°€ì •
    - Unlabeled Dataì™€ Perturbed Unlabeled Dataê°„ì˜ Consistencyë¥¼ ìœ ì§€í•˜ë„ë¡ ëª¨ë¸ í•™ìŠµ
    - â˜› í—·ê°ˆë¦¬ëŠ” Sampleì— ëŒ€í•œ ìœ ì—°í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥  
  

```Temporal Ensemble```ì€ ğ…-Model ë…¼ë¬¸ì—ì„œ ğ…-Modelì˜ í•œê³„ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ê°™ì´ ì œì•ˆí•œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

<p align = 'center'>
<img src = 'https://user-images.githubusercontent.com/56019094/209636928-9c9d7a79-b176-40fe-ac58-03e19ea3f8c8.png' width = 90%>
</p>

- ğ…-Model
    - Inputì— Gaussian Noiseì™€ Stochastic Augmentationì„ ì ìš©í•´ ë‘ ê°€ì§€ Augmented Input êµ¬ì„±  
    - ë™ì¼í•œ Network êµ¬ì¡°ì— ë‹¤ë¥¸ Dropout Regularizationì„ ì ìš©í•˜ì—¬ ëª¨ë¸ êµ¬ì„±
    - Augmentation 1 ëª¨ë¸ì˜ Outputê³¼ Labelë¡œ Cross Entropy Loss ì‚°ì¶œ (Supervised Loss)
    - Augmentation 1 ëª¨ë¸ì˜ Outputê³¼ Augmentation 2 ëª¨ë¸ì˜ Output ê°„ì˜ Consistencyë¥¼ ìœ ì§€í•˜ë„ë¡ MSE Loss ì‚°ì¶œ (Consistency Loss)

    - í•œê³„ì 
        - Networkì˜ Single Evaluationì— ê¸°ë°˜í•´ Training Targetì´ êµ¬ì„±ë˜ê¸°ì— Noisy
    

- Temporal Ensemble  
    >ğŸ¤© Temporal Ensembleì€ ìœ„ì™€ ê°™ì€ ğ…-Modelì˜ í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ Network Evaluation(Output)ë“¤ì„ Ensembleí•˜ì—¬ Target ê°’ìœ¼ë¡œ ì‚¬ìš©

    <p align = 'center'>
    <img src = 'https://user-images.githubusercontent.com/56019094/209637719-78b704f4-435d-4e1f-9d11-59f7987578d3.png' width = 90%>
    </p>

    - ìœ„ Figureì˜ ìˆ˜ë„ ì½”ë“œì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ì‹œí”¼ Temporal Ensembleì€ Network Evaluation(Output)ì„ Ensembleí•˜ê¸° ìœ„í•´ Exponential Moving Average (EMA)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - Outputì˜ EMAë¥¼ Targetìœ¼ë¡œ ì‚¬ìš©í•´ Noiseê°€ ì¤„ì–´ë“ ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, EMAë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ Networkì˜ Outputë“¤ì„ ì¶”ê°€ì ìœ¼ë¡œ ì €ì¥í•´ì•¼ í•œë‹¤ëŠ” ê²ƒê³¼ EMA ìˆ˜ì‹ì— í¬í•¨ë˜ëŠ” $\alpha$ê°€ ì¶”ê°€ì ì¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.  
    ë³¸ Tutorialì—ì„œ ë˜í•œ ì´ $\alpha$ì˜ ê°’ì„ ë³€ê²½í•˜ë©° ëª¨ë¸ì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ ì¸¡ì •í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

<hr/>

## Code

- ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
```python
def mnist_dataset(root, transform):
    # Load Train Data
    train_dataset = datasets.MNIST(
        root=root,
        train=True,
        transform=transform,
        download=True
    )

    # Load Test Data
    test_dataset = datasets.MNIST(
        root=root,
        train=False,
        transform=transform,
        download=True
    )
    return train_dataset, test_dataset
```
ë³¸ Tutorialì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ ê°€ì¥ ë„ë¦¬ ì•Œë ¤ì§„ ë°ì´í„°ì…‹ì¸ MNIST ë°ì´í„°ì…‹ì„ ì´ìš©í–ˆìŠµë‹ˆë‹¤.


- ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
```python
def preprocess_data(train_dataset, test_dataset, batch_size, k, n_classes, seed, shuffle_train=False, return_idx=True):
    # Randomly form unlabeled data in training dataset
    n = len(train_dataset)  # Dataset size
    rand_seed = np.random.RandomState(seed) # Set seed 
    indices = torch.zeros(k)  # Empty tensor for saving indices for keeping labeled data
    unlabel_indices = torch.zeros(n - k)  # Empty tensor for indices of unlabeled data
    quot = k // n_classes 
    temp_index = 0

    for i in range(n_classes):
        class_items = (train_dataset.train_labels == i).nonzero()  # indices of samples with label i
        # train_dataset.train_labels == i : Train Data ì¤‘ Labelì´ iì¸ ê²ƒë“¤ë§Œ True
        # .nonzero(): Element ê°’ì´ 0ì´ ì•„ë‹Œ Elementë“¤ì˜ Indicesë§Œ ë°˜í™˜ (Element ê°’ì´ 0ì´ë©´ .nonzero() ê²°ê³¼ì—ë„ ë°˜í™˜ë˜ì§€ ì•ŠìŒ)

        n_class = len(class_items)  # number of samples with label i
        shuffled = rand_seed.permutation(np.arange(n_class))  # shuffle them  |shuffled| = |n_class|
        indices[i * quot: (i+1) * quot] = torch.squeeze(class_items[shuffled[:quot]]) # |class_items[shuffled[:quot]]| = (alpha, 1)ì´ë¼ì„œ 2ì°¨ì›ì´ë¼ Squeeze ì ìš©
        unlabel_indices[temp_index: temp_index+n_class-quot] = torch.squeeze(class_items[shuffled[quot:]])
        temp_index += (n_class-quot)

    unlabel_indices = unlabel_indices.long() # tensor as indices must be long, byte or bool
    train_dataset.train_labels[unlabel_indices] = -1 # Unsupervisedì˜ ê²½ìš° Label = -1 í• ë‹¹

    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               num_workers=0,
                                               shuffle=shuffle_train)

    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=batch_size,
                                              num_workers=0,
                                              shuffle=False)

    if return_idx:
        return train_loader, test_loader, indices
    return train_loader, test_loader
```

- Input Dataì— ì ìš©í•  Gaussian Noise ì ìš© í´ë˜ìŠ¤ ì •ì˜
```python
class GaussianNoise(nn.Module):
    def __init__(self, batch_size, input_shape, std):
        super(GaussianNoise, self).__init__()
        self.shape = (batch_size,) + input_shape # |self.shape| = (batch_size, input_shape's first element, input_shape's second element, input_shape's third element)
        self.std = std
        self.noise = torch.zeros(self.shape).cuda()

    def forward(self, x):
        self.noise.normal_(mean=0, std=self.std) # torch.normal(mean, std): Returns a tensor of random numbers from separate normal distributions
        return x + self.noise
```
- Labeled Dataì— ì‚¬ìš©í•  Cross Entropy Loss í•¨ìˆ˜ ì •ì˜
```python
def labeled_ce_loss(out, labels):
    cond = (labels >= 0) # ì°¸ê³ : preprocess_data()ì—ì„œ Unlabeled Dataì˜ Label = -1 í• ë‹¹í•¨ 
    labeled_arr = torch.nonzero(cond) # Array of Labeled Sample Index
    num_sup = len(labeled_arr) # Num of Supervised Samples
    
    # Supervised Instance ìˆ˜ê°€ 0ë³´ë‹¤ ë§ë‹¤ë©´, ì¦‰ ì¡´ì¬í•œë‹¤ë©´
    if num_sup > 0:
        labeled_outputs = torch.index_select(input=out, dim=0, index=labeled_arr.view(num_sup)) # labeled_arr.view(num_sup): Flatten() ì—­í• 
        labeled_labels = labels[cond] 
        loss = F.cross_entropy(labeled_outputs, labeled_labels)
        return loss, num_sup
    
    # Supervised Instanceê°€ ì—†ë‹¤ë©´ CE Loss = 0
    loss = torch.tensor([0.], requires_grad=False).cuda()
    return loss, 0 # num_sup == 0 ì´ë©´ lossì™€ 0(num_supì´ ì—†ë‹¤ëŠ” ëœ») ë°˜í™˜
```
ìœ„ í•¨ìˆ˜ê°€ ë³¸ Tutorialì—ì„œ Network Outputì„ Ensembleí•˜ëŠ” ë°©ë²• ë‹¤ìŒìœ¼ë¡œ í•µì‹¬ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. Labeled Dataì™€ Unlabeled Dataë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸°ì— Labeled Dataê°€ ì¡´ì¬í•œë‹¤ë©´, Labeled Dataì— í•´ë‹¹í•˜ëŠ” Dataë§Œ ê³¨ë¼ì„œ Cross Entropy í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ëª¨ìŠµì„ `if num_sup > 0` ë¸”ë¡ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Unlabeled Dataì— ì‚¬ìš©í•  MSE Loss í•¨ìˆ˜ ì •ì˜
```python
def mse_loss(cur_out, ensem_out):
    # Current Outputê³¼ Ensemble Output ê°„ì˜ MSE
    se = torch.sum((F.softmax(cur_out, dim=1) - F.softmax(ensem_out, dim=1)) ** 2)
    
    return se/len(cur_out)
```
Unlabeled Dataì— ëŒ€í•´ì„œ MSE Lossë¥¼ êµ¬í•  ë•Œ Inputìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ë¥¼ ë³´ë©´ Cross Entropy Lossë¥¼ ê³„ì‚°í•  ë•Œì™€ ë‹¬ë¦¬ Labelì´ ì—†ëŠ” ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Supervised Loss, Unsupervised Loss(=Consistency Loss), Total Loss ë°˜í™˜ í•¨ìˆ˜ ì •ì˜
```python
def return_losses(cur_out, ensem_out, w, labels):
    # cur_out: Current output
    # ensem_out: Ensemble output
    # w: Weight for summation loss

    sup_loss, nbsup = labeled_ce_loss(cur_out, labels)
    unsup_loss = mse_loss(cur_out, ensem_out)
    total_loss = sup_loss + w * unsup_loss # ìµœì¢… Loss = Supervised Loss + w*Unsupervised Loss

    return total_loss, sup_loss, unsup_loss, nbsup
```

- Supervised Lossì™€ Unsupervised Lossë¥¼ ë”í•´ Total Loss ê³„ì‚° ì‹œ Unsupervised Loss Termì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ ì •ì˜ (ì•„ë˜ ìˆ˜ì‹ì—ì„œ $w$ì— í•´ë‹¹)

$$L_{total} = wL_u + L_s$$

```python
def weight_ramp_up(epoch:int, max_epochs:int, max_val:float, mult, n_labeled:int, n_samples:int):

    max_val = max_val * (n_labeled/n_samples)

    if epoch == 0:
        return 0.
    elif epoch >= max_epochs:
        return max_val

    return max_val * np.exp(-mult * (1. - float(epoch)/max_epochs)**2)
```
$w$ë¥¼ ë‹¨ìˆœíˆ ì–´ë–¤ ìŠ¤ì¹¼ë¼ ê°’ í•˜ë‚˜ë¡œ ê³ ì •í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ëŠ” ì ì´ ë˜ í•˜ë‚˜ì˜ íŠ¹ì§•ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ í˜•ê´‘íœìœ¼ë¡œ í‘œì‹œí•œ ë¶€ë¶„ì— í•´ë‹¹í•©ë‹ˆë‹¤. í•´ë‹¹ í•¨ìˆ˜ë¥¼ í†µí•´ íŠ¹ì • Epoch(max_epochs) ì´í›„ì—ëŠ” $w(t)$ê°€ íŠ¹ì •í•œ ê°’ìœ¼ë¡œ ê³ ì •ë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.

<p align = 'center'>
<img src = 'https://user-images.githubusercontent.com/56019094/209660612-56f94824-00e5-4117-af6c-a26f349adcf9.png' width = 80%>
</p>

- Network (CNN Model) ëª¨ë¸ ì •ì˜
```python
# Base Modelë¡œ CNN ì‚¬ìš©
class CNN(nn.Module):
    def __init__(self, batch_size:int, std:float, input_shape:tuple = (1,28,28), drop_out:float = 0.5, first_layer:int = 16, second_layer:int = 32):
        super(CNN, self).__init__()
        self.std = std
        self.drop_out = drop_out
        self.first_layer = first_layer
        self.second_layer = second_layer
        self.input_shape = input_shape
        self.conv_block1 = nn.Sequential(nn.Conv2d(1, self.first_layer, 3, stride=1, padding=1),
                                        nn.BatchNorm2d(self.first_layer),
                                        nn.ReLU(),
                                        nn.MaxPool2d(3, stride=2, padding=1))
        self.conv_block2 = nn.Sequential(nn.Conv2d(self.first_layer, self.second_layer, 3, stride=1, padding=1),
                                        nn.BatchNorm2d(self.second_layer),
                                        nn.ReLU(),
                                        nn.MaxPool2d(3, stride=2, padding=1))
        self.drop = nn.Dropout(self.drop_out)
        self.fc = nn.Linear(self.second_layer * 7 * 7, 10)

    def forward(self, x):
        if self.training:
            b = x.size(0)
            noise = GaussianNoise(b, self.input_shape, self.std) # Inputì— Gaussian Noiseë¥¼ ì ìš©í•´ì„œ Augmentation ìˆ˜í–‰
            x = noise(x)

        # first block
        x = self.conv_block1(x)

        # second block
        x = self.conv_block2(x)

        # Classifier (FC)
        x = x.view(-1, self.second_layer * 7 * 7) # Flatten
        x = self.fc(self.drop(x)) # Apply Dropout

        return x
```

- Train í•¨ìˆ˜ ì •ì˜ 
```python
def train(model, 
        train_loader, 
        val_loader,
        k:int, 
        alpha:float, 
        lr:float, 
        num_epochs:int, 
        batch_size:int, 
        n_instances:int, 
        n_classes:int = 10, 
        max_epochs:int = 80, 
        max_val:float = 1.
        ):    
    
    # Feed model to GPU if available
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    model.to(device)

    # Setting Optimizer Adam
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # Set First Ensemble Output as zeros
    Z = torch.zeros(n_instances, n_classes).float().to(device)
    z = torch.zeros(n_instances, n_classes).float().to(device)
    outputs = torch.zeros(n_instances, n_classes).float().to(device)

    losses = [] # Total Loss
    sup_losses = [] # Supervised Loss (Cross-Entropy Loss)
    unsup_losses = [] # Unsupervised Loss (MSE Loss)
    best_loss = 10_000

    for epoch in range(num_epochs):
        model.train()

        # Calculate Unsupervised Loss Weight
        w = weight_ramp_up(epoch, max_epochs, max_val, 5, k, 60000)
        w = torch.tensor(w, requires_grad=False).to(device)

        # Targets change only once per Epoch
        for i, (images, labels) in enumerate(train_loader):
            batch_size = images.size(0)  # retrieve batch size again cause drop last is false
            images = images.to(device)
            labels = labels.requires_grad_(False).to(device)

            optimizer.zero_grad()
            out = model(images)

            # í˜„ì¬ Batchì— í•´ë‹¹í•˜ëŠ” Ensemble ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            z_ = z[i*batch_size: (i+1)*batch_size]
            z_.requires_grad_(False)
            loss, sup_loss, unsup_loss, nbsup = return_losses(out, z_, w, labels)

            # Save outputs
            outputs[i*batch_size: (i+1)*batch_size] = out.detach().clone()
            losses.append(loss.item())
            sup_losses.append(nbsup*sup_loss.item())
            unsup_losses.append(unsup_loss.item())

            # Backpropagation
            loss.backward()
            optimizer.step()

        loss_mean = np.mean(losses)
        sup_loss_mean = np.mean(sup_losses)
        unsup_loss_mean = np.mean(unsup_losses)

        # 5 Epochë§ˆë‹¤ Total Loss, Supervised Loss, Unsupervised Loss ì¶œë ¥
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_mean:.4f}, Supervised Loss: {sup_loss_mean:.4f}, Unsupervised Loss: {unsup_loss_mean:.4f}')
    
        # Modelì˜ Outputsì— EMA ì´ìš©í•´ì„œ Ensemble Outputsìœ¼ë¡œ Update
        Z = alpha * Z + (1. - alpha) * outputs
        z = Z * (1. / (1. - alpha ** (epoch + 1)))

        if loss_mean < best_loss:
            best_loss = loss_mean
            
            print('='*10, f'{epoch + 1} Epoch Model is Saved', '='*10)
            torch.save({'state_dict': model.state_dict()}, f'model_best.pth')
```
Temporal Ensembleì˜ í•µì‹¬ì€ ë°©ë²•ë¡ ì˜ ì´ë¦„ì—ë„ ë‚˜ì™€ìˆë“¯ì´ ë°”ë¡œ Ensembleì„ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì „ì— Ensemble Learning Chapterì—ì„œ ë³´ì•˜ë˜ ë°©ë²•ì—ì„œ ë³´ì•˜ë˜ ë°©ë²•ë“¤ê³¼ ì–´ë–»ê²Œ ë³´ë©´ ì¼ë§¥ìƒí†µí•˜ê²Œ Outputì„ Aggregateí•˜ëŠ” ëª¨ìŠµì„ ìœ„ `train` í•¨ìˆ˜ ì¤‘ ì•„ë˜ ë¸”ë¡ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
```python
# Modelì˜ Outputsì— EMA ì´ìš©í•´ì„œ Ensemble Outputsìœ¼ë¡œ Update
Z = alpha * Z + (1. - alpha) * outputs
z = Z * (1. / (1. - alpha ** (epoch + 1)))
```

- ëª¨ë¸ ì„±ëŠ¥(Accuracy) í‰ê°€ í•¨ìˆ˜ ì •ì˜
```python
def evaluation(model, loader):
    # Evaluation using Best Model
    checkpoint = torch.load('model_best.pth')
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()
    
    correct = 0
    total = 0

    for i, (samples, labels) in enumerate(loader):
        samples = samples.cuda()
        labels = labels.requires_grad_(False).cuda()
        outputs = model(samples)
        _, predicted = torch.max(outputs.detach(), 1)
        total += labels.size(0)
        correct += (predicted == labels.detach().view_as(predicted)).sum()
    
    accuracy = 100 * float(correct) / total
    
    print("="*10, "Evaluation Result", "="*10)
    print(f'Evaluation Result - Accuracy: {accuracy:.2f}')
    return np.round(accuracy,2)  
```

- ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ (ê²°ê³¼ ì¤‘ ì¼ë¶€)
```python
Epoch [50/50], Loss: 0.1354, Supervised Loss: 0.6107, Unsupervised Loss: 0.1115
========== 50 Epoch Model is Saved ==========
========== Evaluation Result ==========
Evaluation Result - Accuracy: 95.48
```

## Mini Experiment
```python
# Modelì˜ Outputsì— EMA ì´ìš©í•´ì„œ Ensemble Outputsìœ¼ë¡œ Update
Z = alpha * Z + (1. - alpha) * outputs
z = Z * (1. / (1. - alpha ** (epoch + 1)))
```
>EMAì—ì„œ í•µì‹¬ì¸ `alpha` ê°’ì„ 0ë¶€í„° 1ê¹Œì§€ ë³€ê²½í•˜ë©° ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ëŠ” ê°„ë‹¨í•œ ì‹¤í—˜ì„ ì¶”ê°€ì ìœ¼ë¡œ ì§„í–‰í•´ë³´ì•˜ìŠµë‹ˆë‹¤. alpha = 0 ~ 0.9ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.  

ğŸ˜… alpha = 1ë¡œ ì„¤ì •í•  ê²½ìš°, `(1. / (1. - alpha ** (epoch + 1)))`ì—ì„œ ì²« ë²ˆì§¸ Epoch = 0ì—ì„œ `1. / 0'ì´ ë˜ì–´ ì—ëŸ¬ê°€ ë°œìƒí•´ í”¼ì¹˜ ëª»í•˜ê²Œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.


<center>

|alpha|Accuracy|
|-----|--------|
|0|94.43
|0.1|94.75
|0.2|94.61
|0.3|95.35
|0.4|95.96
|0.5|94.12
|0.6|95.35
|0.7|94.36
|0.8|95.67
|0.9|94.28
</center>

- ê²°ë¡ 
    - ë³¸ Tutorialì—ì„œ alpha ê°’ì„ ë³€í™”ì‹œí‚¤ë©° ì‹¤í—˜ì„ ì§„í–‰í•œ ê²°ê³¼ Accuracy ìƒì—ì„œ í° ì°¨ì´ê°€ ë°œìƒí•˜ì§€ëŠ” ì•Šì•˜ìœ¼ë‚˜, alpha = 0.7ì¸ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” ì „ë°˜ì ìœ¼ë¡œ alpha = 0.3 ~ 0.8ì¸ ê²½ìš°ê°€ alpha = 0, 0.1, 0.9ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
    - ì´ëŸ¬í•œ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí•œ ì´ìœ ëŠ” Temporal Ensembleì˜ Motivationì¸ Outputì˜ Noiseë¥¼ Network Outputì˜ EMAë¥¼ í†µí•´ ê°ì†Œì‹œí‚¤ëŠ” ê²ƒì´ íš¨ê³¼ê°€ ìˆì—ˆë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.
    - í•œê³„ì   
        1. ë³¸ Tutorialì—ì„œ ì‚¬ìš©ëœ Base Modelì˜ ê²½ìš° íŒŒë¼ë¯¸í„° ê°’ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ íŠ¹ì •í•œ ê°’ì„ ê³ ì •ì‹œí‚¨ ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, Seedì— ë”°ë¼ ì„±ëŠ¥ì— ë³€í™”ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        2. alpha = 1ì¸ ê²½ìš°ë¥¼ ì½”ë“œ ìƒì˜ ë¬¸ì œë¡œ ì¸í•´ ì‹¤í—˜ì„ ì§„í–‰í•´ë³´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.