# SVM
---
**Tutorial for Business Analytics**  
ğŸ˜‰ If you are curious about the time it takes to establish an SVM model by kernel function, I tried some experiment. Check [ì½”ë”© ì‹¤ìŠµ]
- ë³¸ Tutorialì€ ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ëŒ€í•™ì› Business Analytics ê°•ì˜ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
---

## ëª©ì°¨

1. [ì´ë¡ ](#ì´ë¡ )
   
   1. [Margin](#Margin)
   
   2. [Optimization](#Optimization-ë¬¸ì œ)
   
   3. [Soft Margin SVM](#Soft-Margin-SVM)
   
   4. [Nonlinear&Kernel](#Nonlinear&Kernel)
   
2. [ì½”ë”© ì‹¤ìŠµ](#ì½”ë”©-ì‹¤ìŠµ)
   
   1. [ì‹¤í—˜ ì£¼ì œ](#ì‹¤í—˜-ì£¼ì œ)
   
   2. [Main Experiment - Support Vector Classifier](#Main-Experiment---Support-Vector-Classifier)
   
   1. [SVC ê²°ê³¼ ë° í•´ì„](#SVC-ê²°ê³¼-ë°-í•´ì„)
   
   3. [Additional Experiment - Support Vector Regressor](#Additional-Experiment---Support-Vector-Regressor)
   
   1. [SVR ê²°ê³¼ ë° í•´ì„](#SVR-ê²°ê³¼-ë°-í•´ì„)
   
   4. [ê²°ë¡ ](#ê²°ë¡ )
   
---
># **ì´ë¡ **

**S**uppor **V**ector **M**achine

Keywords: Margin, Hyperplane, Support Vector
 

ğŸ“¢ ìš”ì•½: Support Vector Machineì€ Vector Space ìƒì—ì„œ Vectorë“¤ì„ ê°€ì¥ ì˜ ë¶„ë¥˜í•˜ëŠ” Hyperplaneì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.

- Background
    - Hyperplane(ì´ˆí‰ë©´): a subspace of one dimension less than its ambient space
    
      
    <p align="center">
    <image src=https://user-images.githubusercontent.com/56019094/199520467-88e4f48d-11e1-42a2-868b-0ca4ddcf7b56.png
    height="300"/>  
    </p>
    ì´ë¯¸ì§€ ì¶œì²˜: Support Vector Machines without tears
        
    - nì°¨ì›ì˜ ê³µê°„ì—ì„œ Hyperplaneì€ n-1ì°¨ì›ì˜ subspaceë¥¼ ì˜ë¯¸
        - 2ì°¨ì›ì˜ ê²½ìš° Hyperplaneì€ 1ì°¨ì›(ì§ì„ )
        - 3ì°¨ì›ì˜ ê²½ìš° Hyperplaneì€ 2ì°¨ì›(í‰ë©´)  
        â‡’ SVMì—ì„œ Hyperplaneì€ ì–´ë–¤ Vector Space ìƒì— ì¡´ì¬í•˜ëŠ” Vectorë“¤ì„ ë¶„ë¥˜í•˜ëŠ” Decision Boundary(ê²°ì • ê²½ê³„)ì— í•´ë‹¹
        

>## Margin

>>### â€œê°€ì¥ ì˜ ë¶„ë¥˜í•˜ëŠ”â€ì˜ ê¸°ì¤€ì´ ë¬´ì—‡ì¸ê°€?

- SVMì€ Vector Space ìƒì— ìˆëŠ” Vector í˜•íƒœë¡œ í‘œí˜„ëœ ê° Data Pointë“¤ì„ ê°€ì¥ ì˜ ë¶„ë¥˜í•˜ëŠ” Hyperplane(ì´ˆí‰ë©´)ì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
  

<p align = 'center'>
<image src=https://user-images.githubusercontent.com/56019094/199521480-96c01d5c-4bc4-4f18-8273-b4814f0320b6.png height = '300'></p>
    

>>### Marginì„ ìµœëŒ€í™”í•˜ëŠ” Hyperplaneì„ ì°¾ì

- Marginì´ë€?
  
    : Hyperplaneìœ¼ë¡œë¶€í„° ë“± ê°„ê²©ìœ¼ë¡œ ì–‘ìª½ìœ¼ë¡œ í™•ì¥ì‹œì¼°ì„ ë•Œ Hyperplaneê³¼ ê°€ì¥ ê°€ê¹Œìš´ ê°ì²´(Support Vector)ì™€ì˜ ê±°ë¦¬
    
    <p align = 'center'>
    <image src=https://user-images.githubusercontent.com/56019094/199521778-96a75042-461b-4104-98e3-325bd3c33065.png height = '300'></p>
    
    - ìœ„ì˜ Hyperplane ë„¤ ê°œ ëª¨ë‘ ë‹¤ ë°ì´í„°ë“¤ì„ ì˜ ë¶„ë¥˜í•˜ëŠ”ë° Marginì´ í¬ë©´ ë­ê°€ ì¢‹ì„ê¹Œ?
      
        **Margin ìµœëŒ€í™” â†’ êµ¬ì¡°ì  ìœ„í—˜ ìµœì†Œí™”**
        
        Equation 1)
        
        $$
        h \le min(\lceil {{R^2 \over \triangle^2}}\rceil, D) + 1
        $$
        
        Equation 2)
        
        $$
        R[f] \le R_{emp}[f] + \sqrt{{h{(ln{2n\over h}}+1)-ln({\delta \over 4}) }\over{n}}
        $$
        
        $notation$
        
        $h$: VC Dimension
        
        $R$: ëª¨ë“  Input Vectorë“¤ì„ í¬í•¨í•˜ëŠ” ê°€ì¥ ì‘ì€ êµ¬
        
        $\triangle$: Margin
        
        $D$: Input Spaceì˜ ì°¨ì›(= ë³€ìˆ˜ ê°œìˆ˜)
        
        - Equation 1)ì—ì„œ Dì™€ Rì€ Input Dataê°€ ì£¼ì–´ì§€ë©´ ì •í•´ì§„ ê°’ìœ¼ë¡œ, ë³€í™” ê°€ëŠ¥í•œ ê²ƒì€ $\triangle$ (Margin)
            - $\triangle$(Margin) ì¦ê°€ â†’ $R^2 \over \triangle^2$ ê°ì†Œ â†’  $min(\lceil{R^2 \over \triangle^2}, D\rceil)$ ê°ì†Œ â†’ $h$  (VC Dimension) ê°ì†Œ
              
                â†’ $\sqrt{{h{(ln{2n\over h}}+1)-ln({\delta \over 4}) }\over{n}}$ (Capacity Term) ê°ì†Œ â†’ $R[f]$ (êµ¬ì¡°ì  ìœ„í—˜) ê°ì†Œ
                

>>### Marginì„ ì–´ë–»ê²Œ ê³„ì‚°í•  ê²ƒì¸ê°€?

<p align = 'center'>
<image src=https://user-images.githubusercontent.com/56019094/199521970-0e80400c-3430-4f84-9e91-2f041f01882a.png height = '300'> </p>

Hyperplaneì„ $\boldsymbol{w}^T\boldsymbol{x} + b$

where $\boldsymbol{w} = (w_1,w_2)^T$ ë¼ê³  ê°€ì •

- ë²¡í„° $\boldsymbol{w}$ëŠ” ì´ Hyperplaneê³¼ ìˆ˜ì§ì¸ ë²•ì„  ë²¡í„°
- $\boldsymbol{w}$ì— ëŒ€í•´ ì›ì ê³¼ì˜ ê±°ë¦¬ê°€ $b$ì¸ ì§ì„ ì˜ ë°©ì •ì‹ì€ $\boldsymbol{w}^T\boldsymbol{x} + b = 0$  â‡’ $w_1x_1 + w_2x_2 + b = 0$
- ìœ„ ì§ì„ ì˜ ê¸°ìš¸ê¸°ëŠ” $- {w_1\over w_2}$ì´ê³ , ë²•ì„  ë²¡í„° $\boldsymbol{w}$ì˜ ê¸°ìš¸ê¸°ëŠ” $w_2 \over w_1$ â‡’ ë‘ ì§ì„ ì€ ì§êµ

â‡’ Plus-plane ìœ„ì— ìˆëŠ” ë²¡í„° $\boldsymbol{x}^+$ì™€ Minus-plane ìœ„ì— ìˆëŠ” ë²¡í„° $\boldsymbol{x}^-$ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ê°€ëŠ¥
- $\boldsymbol{x}^+ = \boldsymbol{x}^- + \lambda \boldsymbol{w}$
    - ìœ„ ìˆ˜ì‹ì€ $\boldsymbol{x}^-$ë¥¼ $\boldsymbol{w}$ ë°©í–¥ìœ¼ë¡œ $\lambda$ë§Œí¼ í‰í–‰ì´ë™ì‹œí‚¨ë‹¤ëŠ” ì˜ë¯¸
- $\lambda$ëŠ” ê³„ì‚°í•  ìˆ˜ ìˆì„ê¹Œ?
  
    : $\boldsymbol{w}^T\boldsymbol{x}^+ + b = 1$
    
    â†’ $\boldsymbol{w}^T(\boldsymbol{x}^- + \lambda\boldsymbol{w}) + b = 1$
    
    â†’ $\boldsymbol{w}^T\boldsymbol{x}^- + b + \lambda \boldsymbol{w}^T\boldsymbol{w} = 1$          where, $(\boldsymbol{w}^T\boldsymbol{x}^- + b = 1)$
    
    â†’  $-1 + \lambda\boldsymbol{w}^T\boldsymbol{w} = 1$
    
    â‡’ $\lambda = {2 \over \boldsymbol{w}^T\boldsymbol{w}}$ 
    

í•œí¸, Marginì€ Plus-planeê³¼ Minus-plane ì‚¬ì´ì˜ ê±°ë¦¬ $distance(\boldsymbol{x}^+, \boldsymbol{x}^-)$ì™€ ê°™ìŒ

$Margin = distance(\boldsymbol{x}^+, \boldsymbol{x}^-)$

$= ||\boldsymbol{x}^+ - \boldsymbol{x}^-||_2$

$= ||\boldsymbol{x}^- + \lambda\boldsymbol{w}- \boldsymbol{x}^-||_2$ , where, $\boldsymbol{x}^+ = \boldsymbol{x}^- + \lambda \boldsymbol{w}$

$= ||\lambda\boldsymbol{w}||_2$

$= \lambda \sqrt{\boldsymbol{w}^T\boldsymbol{w}}$

 $= {2 \over \boldsymbol{w}^T\boldsymbol{w}}\sqrt{\boldsymbol{w}^T\boldsymbol{w}}$ , where, $\lambda = {2 \over \boldsymbol{w}^T\boldsymbol{w}}$

$= {2 \over \sqrt{\boldsymbol{w}^T\boldsymbol{w}}}$

$= {2 \over ||w||_2}$

>## Optimization ë¬¸ì œ

**Remind!** SVMì˜ ëª©ì ì€ **Margin**ì„ **ìµœëŒ€**ë¡œ í•˜ëŠ” Hyperplaneì„ ì°¾ëŠ” ê²ƒ

>>### ëª©ì  í•¨ìˆ˜ ë° ì œì•½ ì¡°ê±´

- Marginì„ ìµœëŒ€í™”:  $max$  ${2 \over ||w||^2}$    --ì—­ìˆ˜->     $min$   ${1 \over 2}||w||^2$
  
    $$min   {1 \over 2}||w||^2$$
    
    $$s.t.  \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge 1 \quad   , \forall i$$
    
    <p align = 'center'>
    <img src = https://user-images.githubusercontent.com/56019094/199522537-dcbdf18f-d3d0-4e16-8130-23c6f1a73f14.png height = '300'></p>
    
    - Let $\boldsymbol{x}_i$ = íŒŒë€ìƒ‰ Data Object, $\boldsymbol{x}_j$ = ë¹¨ê°„ìƒ‰ Data Object
        - $\boldsymbol{w}\boldsymbol{x}_i \ge 1$      $(y_i = +1)$  â†’   $y_i(\boldsymbol{w} \cdot \boldsymbol{x}_i + b) \ge +1$
        - $\boldsymbol{w}\boldsymbol{x}_j \le -1$  $(y_j = -1)$ â†’   $y_j(\boldsymbol{w} \cdot \boldsymbol{x}_j + b ) \ge +1$
        
        â‡’ $y$ = $\pm 1$ì¸ ê²½ìš° ëª¨ë‘, ìˆ˜ì‹ì´ ë™ì¼í•˜ê²Œ ìœ„ ì œì•½ ì¡°ê±´ê³¼ ê°™ì´ ì •ë¦¬ë¨ 
            (Plus. SVMì—ì„œ Class Labelì„ 0/1ì´ ì•„ë‹Œ +1/-1ë¡œ ì„¤ì •í•œ ì´ìœ ) 
        
    

>>### ë¼ê·¸ë‘ì§€ì•ˆ ë¬¸ì œë¡œ ë³€í™˜

- ê¸°ì¡´ ëª©ì  í•¨ìˆ˜ ë° ì œì•½ ì¡°ê±´
  
    $$min \quad {1 \over 2}||w||^2$$
    
    $$s.t. \quad   y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge 1 \quad   , \forall i$$
    
    â‡’ ìœ„ ì‹ì—ì„œ $y_i, \boldsymbol{x}_i$ëŠ” ì£¼ì–´ì§„ ê°’ì´ê³ , $\boldsymbol{w}$ì™€ $b$ê°€ ë¯¸ì§€ìˆ˜ ì¦‰, ìµœì í™” ëŒ€ìƒ
    
- ë¼ê·¸ë‘ì§€ì•ˆ ë¬¸ì œ
  
    $$\min L_p\left(\mathbf{w}, b, \alpha_i\right)=\frac{1}{2}\|\mathbf{w}\|^2-\sum_{i=1}^N \alpha_i\left(y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right)-1\right)$$  
    $$s.t.$   $\alpha_i \ge 0$$

>>### ìŒëŒ€(Dual) ë¬¸ì œë¡œ ë³€í™˜

- KKT ì¡°ê±´
  
    $${\partial L_p \over \partial \boldsymbol{w}} = 0 \quad   â‡’  \quad \boldsymbol{w} = \sum_{i=1}^N {\alpha_iy_i\boldsymbol{x}_i}$$  
    
    $${\partial L_p \over  \partial b} = 0 \quad   â‡’  \quad  \sum_{i=1}^N {\alpha_iy_i} = 0$$
    
- ì›ë¬¸ì œ
  
    $$\min L_p\left(\mathbf{w}, b, \alpha_i\right)=\frac{1}{2}\|\mathbf{w}\|^2-\sum_{i=1}^N \alpha_i\left(y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right)-1\right)$$  
    $$s.t. \quad   \alpha_i \ge 0$$ 

- ìŒëŒ€(Dual) ë¬¸ì œ
  
    $$\max L_D\left(\alpha_i\right)=\sum_{i=1}^N \alpha_i-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j$$    
    
    $$s.t. \quad \sum_{i=1}^N \alpha_i y_i=0, \quad \alpha_i \geq 0$$
  
    
    
    
    - ìœ„ ìŒëŒ€ ë¬¸ì œì—ì„œ $\boldsymbol{x}$ì™€ $y$ëŠ” ë°ì´í„°ë¡œë¶€í„° ì£¼ì–´ì§„ ê°’ì´ê³  $\alpha$ë§Œ ë¯¸ì§€ìˆ˜
    - ì´ë•Œ, KKT ì¡°ê±´ì— ë”°ë¼ $\alpha_i(y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1) = 0$ ì´ë¼ëŠ” ìˆ˜ì‹ì´ ì„±ë¦½í•¨
      
        â‡’ $\alpha_i = 0, (y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1) \ne 0$ ì´ê±°ë‚˜ $\alpha_i \ne 0, (y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1) = 0$
        
        - $\alpha_i \ne 0, (y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1) = 0$ì¸ ê²½ìš°,
          
            $(y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1) = 0$   â†’   $y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) = 1$ì´ë¼ëŠ” ê²ƒì€ 
            
            $\boldsymbol{x}_i$ê°€ Plus-planeê³¼ Minus-plane ìƒì— ìœ„ì¹˜í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸
            â†’ ì´ $\boldsymbol{x}_i$ ( = Support Vector)ì— ëŒ€í•´ì„œë§Œ $\alpha_i$ëŠ” 0ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ê²Œ ë¨.
            
            <p align = 'left'><img src = https://user-images.githubusercontent.com/56019094/199523250-335b3594-beef-4cba-8598-fe7931fdf682.png height = '300'></p>
            ì´ë¯¸ì§€ ì¶œì²˜: [https://techblog-history-younghunjo1.tistory.com/m/78]
            

>>### ìµœì¢… Hyperplane êµ¬í•˜ê¸°

- SVMì—ì„œ ì°¾ê³ ì í•˜ëŠ” ê²ƒì€ Marginì´ ìµœëŒ€í™”ëœ Hyperplane $\boldsymbol{w}^T\boldsymbol{x} + b$
  
    â†’ $\boldsymbol{w}$ì™€ $b$ë¥¼ ì°¾ìœ¼ë©´ Hyperplane êµ¬í•  ìˆ˜ ìˆìŒ
    
- ì´ì „ ë‹¨ê³„ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì—ˆìŒ
  
    $$\boldsymbol{w} = \sum_{i=1}^N {\alpha_iy_i\boldsymbol{x}_i}$$
    
    - $\boldsymbol{x}_i, y_i$ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ì•Œì•„ë‚¼ ìˆ˜ ìˆëŠ” ê°’ì´ë¯€ë¡œ ë‹¨ í•˜ë‚˜ë¿ì¸ ë¯¸ì§€ìˆ˜ì¸ $\alpha$ë¥¼ ì•Œë©´ $\boldsymbol{w}$ë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŒ
    - $\boldsymbol{w}$ë¥¼ êµ¬í•œ ë’¤, $(y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1) = 0$ì„ í†µí•´ $b$ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
- $\boldsymbol{x}_{new}$ (ìƒˆë¡œìš´ Instance)ê°€ ë“¤ì–´ì˜¤ë©´ ìœ„ ìˆ˜ì‹ì— ë„£ì–´ì„œ ê·¸ ê°’ì´ 0ë³´ë‹¤ í¬ë©´ Class Labelì„ +1ë¡œ, ê°’ì´ 0ë³´ë‹¤ ì‘ìœ¼ë©´ Class Labelì„ -1ë¡œ ì˜ˆì¸¡í•¨

>## Soft Margin SVM

- ì´ì „ê¹Œì§€ ì„¤ëª…í•œ SVMì€ Hyperplaneê³¼ Support Vectors ì‚¬ì´ì— Instanceê°€ ì¡´ì¬í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” Hard Margin SVMì´ì—ˆìŒ
- Soft Margin SVMì€ Hyperplaneê³¼ Support Vectors ì‚¬ì´ì— ì–´ëŠì •ë„ Instanceê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ í—ˆìš©

>>### ëª©ì  í•¨ìˆ˜ ë° ì œì•½ ì¡°ê±´

$$min \quad {1 \over 2}||\boldsymbol{w}||^2 +C\sum_{i=1}^N \xi_i$$  

$$s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge 1-\xi_i, \quad \xi_i \ge0, \forall \quad i$$

<p align = 'left'><img src = https://user-images.githubusercontent.com/56019094/199524039-91704ab9-95e5-40be-a78e-83a4f7151e5a.png height = '250'></p>

$notation$

$C$: Penaltyì˜ ì •ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” Hyperparameter

$\xi$: Penalty

â‡’ ë¯¸ì§€ìˆ˜: $\boldsymbol{w}, b, \xi$

>>### ë¼ê·¸ë‘ì§€ì•ˆ ë¬¸ì œë¡œ ë³€í™˜

$$\min L_p\left(\mathbf{w}, b, \alpha_i\right)=\frac{1}{2}\|\mathbf{w}\|^2+C \sum_{i=1}^N \xi_i-\sum_{i=1}^N \alpha_i\left(y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right)-1+\xi_i\right)-\sum_{i=1}^N \mu_i \xi_i$$  

$$s.t.\quad\alpha_i \ge 0$$

>>### ìŒëŒ€(Dual) ë¬¸ì œë¡œ ë³€í™˜

- ì›ë¬¸ì œ

$$\min L_p\left(\mathbf{w}, b, \alpha_i\right)=\frac{1}{2}\|\mathbf{w}\|^2+C \sum_{i=1}^N \xi_i-\sum_{i=1}^N \alpha_i\left(y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right)-1+\xi_i\right)-\sum_{i=1}^N \mu_i \xi_i$$  


$$s.t.\quad\alpha_i \ge 0$$

- KKT ì¡°ê±´
  
    $${\partial L_p \over {\partial \boldsymbol{w}}} = 0 \quad â‡’  \quad   \boldsymbol{w} = \sum_{i=1}^n\alpha_iy_i\boldsymbol{x}_i$$  
    
    $${\partial L_p \over \partial b} = 0 \quad â‡’ \quad     \sum_{i=1}^n\alpha_iy_i = 0$$  
    
    $${\partial L_p \over \partial \xi_i} = 0 \quad â‡’ \quad    C - \alpha_i - \mu_i = 0$$
    

 $$ â‡’ \quad L_D = {1 \over 2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\cdot\boldsymbol{x}_j + C\sum_i \xi_i -\sum_i\sum_j\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\cdot\boldsymbol{x}_j - b\sum_i\alpha_iy_i + \sum_i\alpha_i - \sum_i\alpha_i\xi_i - \sum_i\mu_i\xi_i$$ 

 $$ â†’ \quad \sum_i(C-\alpha_i-\mu_i)\xi_i = 0, \quad \sum_i\alpha_iy_i = 0$$  

 $$ â†’ \quad L_D = {1 \over 2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\cdot\boldsymbol{x}_j -\sum_i\sum_j\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\cdot\boldsymbol{x}_j + \sum_i\alpha_i$$   

 $$ â†’ \quad L_D = \sum_i\alpha_i  - {1 \over 2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\cdot\boldsymbol{x}_j$$

 $$ â‡’ \quad L_D({\alpha_i}) = \sum_{i=1}^N\alpha_i  - {1 \over 2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j$$

- ìŒëŒ€ ë¬¸ì œ

$$max\quad L_D({\alpha_i}) = \sum_{i=1}^N\alpha_i  - {1 \over 2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j$$    


$$s.t.\quad \sum_{i=1}^N \alpha_iy_i = 0, 0 \le \alpha_i \le C$$



>>### Plus) $\alpha_i$ ê°’ì— ë”°ë¥¸ Instance ìœ„ì¹˜

KKT ì¡°ê±´ìœ¼ë¡œë¶€í„° $\alpha_i(y_i(\boldsymbol{w}^T\boldsymbol{x} + b)-1+\xi_i) = 0$ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŒ
- Support Vectorì— ëŒ€í•´ì„œë§Œ $\alpha_i \ne 0$ì´ ì„±ë¦½

ë˜í•œ $C - \alpha_i - \mu_i = 0, \mu_i\xi_i = 0$ì´ë¼ëŠ” ìˆ˜ì‹ì´ ì„±ë¦½í•¨
- **Case 1)** $\alpha_i = 0\quad$â‡’ Support Vectorê°€ ì•„ë‹Œ Instance
- **Case 2)** $0<\alpha_i < C\quad$
  â†’  $\mu_i < C$ì´ë©´ $C - \alpha_i - \mu_i = 0$ì´ ì„±ë¦½í•˜ê¸° ìœ„í•´ $\mu_i > 0$ ì´ì–´ì•¼ í•¨. 
  â†’ $\mu_i > 0$ì´ë¼ë©´ $\mu_i\xi_i = 0$ì´ ì„±ë¦½í•˜ê¸° ìœ„í•´ $\xi_i = 0$ì´ì–´ì•¼ í•¨
  
    â†’ $y_i(\boldsymbol{w}^T\boldsymbol{x} + b)-1 = 0$ì¸ Instance
  
    â‡’ Margin ìœ„ì— ìœ„ì¹˜í•˜ëŠ” Support Vector
  
- **Case 3)** $\alpha_i = C \quad$
  â†’ $C - \alpha_i - \mu_i = 0$ì—ì„œ $\alpha_i = C$ë¼ë©´ $\mu_i = 0$
  
    â†’ $\mu_i = 0$ì´ë¼ë©´ $\xi_i > 0$
  
    â‡’ Margin ë°–ì— ìœ„ì¹˜í•˜ëŠ” Support Vector
  
    <p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199524486-539f4d6a-37fe-4882-bbf7-97188c7d06d2.png height = '300'></p>
  

Hyperparameter C(ì˜¤ë¶„ë¥˜ ë¹„ìš©)ì— ë”°ë¥¸ ë¶„ë¥˜ ê²½ê³„ë©´ ë³€í™”

<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199524779-72b14258-12fb-4d6b-a601-8bf87d5f069d.png height = '300'></p>

$$
min \quad {1 \over 2}||\boldsymbol{w}||^2 +C\sum_{i=1}^N \xi_i
$$

Large C: ëª©ì í•¨ìˆ˜ì—ì„œ Penaltyê°€ ë” í° ì˜í–¥ë ¥ì„ ê°€ì§   
    â†’ Penaltyë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰  
    â†’ Marginì´ ì¢ê³ , $\alpha_i = C$ì¸ Support Vectorì˜ ìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ìŒ  

Small C: ëª©ì í•¨ìˆ˜ì—ì„œ Penaltyì˜ ì˜í–¥ë ¥ì´ ì‘ì•„ì§   
      â†’ Penaltyì˜ ì˜í–¥ë ¥ì´ ì ìœ¼ë¯€ë¡œ Marginì„ ì¡°ê¸ˆ ë” ë„“ê²Œ ì¡ì„ ìˆ˜ ìˆìŒ  
      â†’ $\alpha_i = C$ì¸ Support Vectorì˜ ìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë§ìŒ  

>## Nonlinear&Kernel

- Linear Modelì˜ í•œê³„: ë¶„ë¥˜ ê²½ê³„ë©´ì´ ë¹„ì„ í˜•ì¼ ê²½ìš° ì˜ ì°¾ì•„ë‚´ì§€ í•¨
  
    <p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199524989-6ae5a362-d1ba-4a01-b749-1e0be33d5296.png height = '300'></p>
    


ğŸ§ ì„ í˜• ë¶„ë¥˜ê°€ ê°€ëŠ¥í•œ ê³ ì°¨ì›ìœ¼ë¡œ ë°ì´í„°ë¥¼ Mappingí•´ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ì!

<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199525195-e5ee6860-9c28-48cf-a3fd-f86223d3b91c.png height = '300'></p>
ì´ë¯¸ì§€ ì¶œì²˜: [https://towardsdatascience.com/support-vector-machine-formulation-and-derivation-b146ce89f28]

â‡’ ê³ ì°¨ì› Mappingì„ í†µí•´ Nonlinear(ë¹„ì„ í˜•) ë¶„ë¥˜ ê²½ê³„ë©´ ìƒì„±

>>### ê³ ì°¨ì›ì—ì„œì˜ ëª©ì  í•¨ìˆ˜ ë° ì œì•½ ì¡°ê±´

$$min\quad{1 \over 2}||\boldsymbol{w}||^2 + C\sum_{i=1}^N\xi_i$$  


$$s.t\quad y_i(\boldsymbol{w}^T\Phi(\boldsymbol{x}_i) + b) \ge 1-\xi_i,\quad \xi_i \ge0, \quad\forall i$$

â‡’ **ë¼ê·¸ë‘ì§€ì•ˆ ë¬¸ì œë¡œ ë³€í™˜**

$$\min L_p\left(\mathbf{w}, b, \alpha_i\right)=\frac{1}{2}\|\mathbf{w}\|^2+C \sum_{i=1}^N \xi_i-\sum_{i=1}^N \alpha_i\left(y_i\left(\mathbf{w}^T \Phi\left(\mathbf{x}_i\right)+b\right)-1+\xi_i\right)-\sum_{i=1}^N \mu_i \xi_i$$  
- KKT ì¡°ê±´
  
    $${\partial L_P \over \partial w} = 0\quad â‡’ \quad \boldsymbol{w} = \sum_{i=1}^n\alpha_iy_i\Phi(\boldsymbol{x}_i)$$  
    
    $${\partial L_P \over \partial b} = 0\quad â‡’ \quad \sum_{i=1}^n\alpha_iy_i = 0$$  
    
    $${\partial L_P \over \partial \xi_i} = 0\quad â‡’ \quad C - \alpha_i - \mu_i = 0$$
    

â‡’ ìŒëŒ€(Dual) ë¬¸ì œë¡œ ë³€í™˜

$$max\quad L_D({\alpha_i}) = \sum_{i=1}^N\alpha_i  - {1 \over 2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\Phi(\boldsymbol{x}_i)^T\Phi(\boldsymbol{x}_j)$$  

$$s.t.\quad \sum_{i=1}^N \alpha_iy_i = 0, \quad 0 \le \alpha_i \le C$$

ğŸ˜“ ê³ ì°¨ì›ìœ¼ë¡œ Mappingì‹œí‚¤ëŠ” í•¨ìˆ˜ $\Phi$ë¥¼ ì–´ë–»ê²Œ ì°¾ì„ê¹Œ,,,?

ğŸ‘ğŸ» Kernel Trickì„ ì“°ì!

$$max\quad L_D({\alpha_i}) = \sum_{i=1}^N\alpha_i  - {1 \over 2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\Phi(\boldsymbol{x}_i)^T\Phi(\boldsymbol{x}_j)$$

  $$ â‡’ max\quad L_D({\alpha_i}) = \sum_{i=1}^N\alpha_i  - {1 \over 2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j K(\boldsymbol{x}_i, \boldsymbol{x}_j)$$

### Kernel Trick

$$max\quad L_D({\alpha_i}) = \sum_{i=1}^N\alpha_i  - {1 \over 2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\Phi(\boldsymbol{x}_i)^T\Phi(\boldsymbol{x}_j)$$  
ìœ„ ìˆ˜ì‹ì—ì„œì™€ ê°™ì´ ê³ ì°¨ì›ì—ì„œëŠ” í•­ìƒ $\Phi({\boldsymbol{x}_i})^T\Phi(\boldsymbol{x}_j)$ì™€ ê°™ì´ ë²¡í„°ì˜ ë‚´ì  í˜•íƒœë¡œ ì¡´ì¬

â†’ ì €ì°¨ì› ë°ì´í„°ë¥¼ ì…ë ¥ ë°›ì•„ì„œ ê³ ì°¨ì› ê³µê°„ìƒì— ë‚´ì  ê²°ê³¼ê°’ì„ ì¤„ ìˆ˜ ìˆë‹¤ë©´ êµ³ì´ $\Phi$ë¥¼ ì°¾ì§€ ì•Šì•„ë„ ëœë‹¤!

<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199525449-05428317-64ad-4ac0-bbc5-266015f5d493.png height = '300'></p>

- ìœ íš¨í•œ Kernel í•¨ìˆ˜ì˜ ì¡°ê±´
    - Symmetric Matrix
    - Positive semi-definite Matrix

- ëŒ€í‘œì ì¸ Kernel í•¨ìˆ˜
    - Polynomial
      
        $K(x,y) = (x \cdot y + c)^d,\quad c>0$
        
    - Gaussian (RBF)
      
        $K(x,y) = exp(-{||x-y||^2 \over 2\sigma^2}),\quad \sigma \ne 0$
        
    - Sigmoid
      
        $K(x,y) = tanh(a(x\cdot y)+b),\quad a,b\ge0$
    
- Kernel í˜•íƒœì— ë”°ë¥¸ ë¶„ë¥˜ ê²½ê³„ë©´
    - Linear Kernel: ì„ í˜• ë¶„ë¥˜ ê²½ê³„ë©´ë§Œ ìƒì„± ê°€ëŠ¥
    - Non-linear Kernel: ë³µì¡í•œ í˜•íƒœì˜ ë¶„ë¥˜ ê²½ê³„ë©´ ìƒì„± ê°€ëŠ¥
    
    
    <p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199525684-704acd32-74e6-4432-85ab-8d1c778c7dff.png height = '500'></p>
    ì´ë¯¸ì§€ ì¶œì²˜: [https://towardsdatascience.com/multiclass-classification-with-support-vector-machines-svm-kernel-trick-kernel-functions-f9d5377d6f02]



># **ì½”ë”© ì‹¤ìŠµ**

## ì‹¤í—˜ ì£¼ì œ
- Support Vector Machineì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” Kernel Functionì¸ (Linear), Sigmoid, Poly, RBF **Kernel ë³„ë¡œ SVM ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ë˜ëŠ” ì‹œê°„**ì— ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
- í•´ë‹¹ ì£¼ì œ ì„ ì • ë°°ê²½  
    1. Stackoverflow, Github ë“± ì†ŒìŠ¤ ì½”ë“œ ê³µìœ  ì‚¬ì´íŠ¸ì—ëŠ” Support Vector Machine ê´€ë ¨ ì‹¤ìŠµ ì½”ë“œ ì¤‘ Kernel, C ë“±ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë³€ê²½ì— ë”°ë¥¸ ëª¨ë¸ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ í™•ì¸í•˜ëŠ” ê²½ìš°ëŠ” ë‹¤ìˆ˜ ì¡´ì¬
    2. Kernel Function ë³„ ëª¨ë¸ ìˆ˜ë¦½ ì‹œê°„ì— ì†Œìš”ë˜ëŠ” ì‹œê°„ì„ ë¹„êµí•œ ì‹¤í—˜ì€ ë§¤ìš° ë“œë¬¼ì—ˆìŒ. 
    3. Kernel Function ìˆ˜ì‹ì„ ë³´ë©´ $d$ìŠ¹ ê³„ì‚°ì´ í¬í•¨ë˜ëŠ” Polynomial Kernel Functionì´ ê°€ì¥ í•™ìŠµì— ì˜¤ë˜ ì†Œìš”ë˜ê³  RBF, Sigmoid, Linearì´ í›„ìˆœì¼ ê²ƒì´ë¼ ì¶”ì¸¡ë˜ì—ˆìœ¼ë‚˜, ì´ì— ëŒ€í•´ ì‹¤í—˜ì„ í†µí•´ í™•ì¸í•˜ê³ ì í•¨

- â—ï¸ Remind  
    - **Polynomial**
      
        $K(x,y) = (x \cdot y + c)^d,\quad c>0$
        
    - **Gaussian (RBF)**
      
        $K(x,y) = exp(-{||x-y||^2 \over 2\sigma^2}),\quad \sigma \ne 0$
        
    - **Sigmoid**
      
        $K(x,y) = tanh(a(x\cdot y)+b),\quad a,b\ge0$
    
    - Kernel í˜•íƒœì— ë”°ë¥¸ ë¶„ë¥˜ ê²½ê³„ë©´  
      - Linear Kernel: ì„ í˜• ë¶„ë¥˜ ê²½ê³„ë©´ë§Œ ìƒì„± ê°€ëŠ¥
      - Non-linear Kernel: ë³µì¡í•œ í˜•íƒœì˜ ë¶„ë¥˜ ê²½ê³„ë©´ ìƒì„± ê°€ëŠ¥

- ì‹¤í—˜ ë‚´ìš©  
    1. Support Vector Classifierë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ Kernel ë³„ ëª¨ë¸ ìˆ˜ë¦½ ì‹œê°„ ë¹„êµ  
    2. ì‚¬ì´í‚·ëŸ° ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” make_moons, make_gaussian_quantiles ì„ ì´ìš©  
        - make_moons data: Binary Classë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ê° Label ë³„ë¡œ Data Instanceê°€ ì´ˆìŠ¹ë‹¬ í˜•íƒœë¡œ ë¶„í¬í•¨
        ```python
        # ë°ì´í„°ì…‹ ì´í•´ë¥¼ ìœ„í•œ Moon datasets ì‹œê°í™”
        X,y = make_moons(n_samples = 3000, noise = 0.2, random_state = 1002)
        plt.scatter(X[:,0],X[:,1], marker = "o", c = y, s = 80, edgecolor = 'k', linewidth = 1)
        plt.xlabel("$X_1$")
        plt.ylabel("$X_2$")
        plt.show()
        ```
        <p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199547509-7c23f09f-caef-418f-af3b-d8788cac4016.png height = '300'> </p>  
        

        - make_gaussian_quantiles: Binary Classë¡œ ì„¤ì •í–ˆìœ¼ë©° 2ì°¨ì› ìƒì—ì„œ ê° Label ë³„ë¡œ Data Instanceê°€ ë“±ê³ ì„  í˜•íƒœë¡œ ë¶„í¬
        ```python
        # ë°ì´í„°ì…‹ ì´í•´ë¥¼ ìœ„í•œ Gaussian Quantile datasets ì‹œê°í™” 
        # make_gaussian_quantiles()ëŠ” ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜ë¥¼ n_featuresë¥¼ ì´ìš©í•´ ì„¤ì • ê°€ëŠ¥
        X,y = make_gaussian_quantiles(n_samples = 400, n_features = 2, n_classes = 2, random_state = 42)  # 
        plt.scatter(X[:,0],X[:,1], marker = 'o', c = y, s = 60, edgecolor = 'k', linewidth = 1)
        plt.xlabel("$X_1$")
        plt.ylabel("$X_2$")
        plt.show()
        ```
        <p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199548436-2f62154c-3053-42d1-a33a-adcbccc447c6.png height = '300'></p>
        
    3. 2. ì‹¤í—˜ ê³¼ì •ì—ì„œ ê° Kernel ë³„ ë¶„ë¥˜ ì„±ëŠ¥(Accuracy, Recall, Precision, F1-Score) ë° ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ëœ ì‹œê°„ ì¸¡ì •
    4. make_gaussian_quantiles ë°ì´í„°ì—ì„œ ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜(n_features)ë¥¼ ëŠ˜ë ¤ê°€ë©° ê° Kernel Function ë³„ ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ëœ ì‹œê°„ ì¸¡ì •  
    (ê³„ê¸°: ì‹¤í—˜ ì¤‘ make_gaussian_quantiles()ì˜ n_featuresì— ì„ì˜ì˜ ìˆ«ìë¥¼ ëŒ€ì…í•´ í•™ìŠµ ì‹œê°„ì„ í™•ì¸í•˜ëŠ” ê³¼ì •ì—ì„œ Kernel Functionì— ë”°ë¼ ì¼ê´€ëœ ì–‘ìƒì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒí–ˆìœ¼ë‚˜ ì´ì™€ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ í™•ì¸í•¨)
    5. ì¶”ê°€ì ìœ¼ë¡œ SVRì—ì„œë„ make_regression() í•¨ìˆ˜ë¥¼ ì´ìš©í•´ n_featuresë¥¼ ëŠ˜ë ¤ê°€ë©° Kernel Functionì— ë”°ë¥¸ í•™ìŠµ ì‹œê°„ ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì‹¤í—˜í•¨


## *Main Experiment - Support Vector Classifier*
```python
# ì½”ë“œ ì‹¤ìŠµì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ë° ë©”ì†Œë“œ Import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import warnings

from sklearn.svm import SVC, SVR
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons, make_gaussian_quantiles
from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score
from mpl_toolkits.mplot3d import Axes3D

warnings.filterwarnings(action='ignore') 
```
- ì‹¤í—˜ì— ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ ì •ì˜
```python
# Classfication ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜
def metric(model, test_X, test_y):
    preds = model.predict(test_X) # ìˆ˜ë¦½ëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ (Binary)
    acc = accuracy_score(test_y,preds) # Accuracy
    f1 = f1_score(test_y,preds,average='macro') # F1-score
    precision = precision_score(test_y,preds,average='macro') # Precision
    recall  = recall_score(test_y,preds,average='macro') # Recall
    return [acc,precision,recall,f1]
```
```python
# Kernel ë³€ê²½ì— ë”°ë¥¸ SVM ëª¨ë¸ ìˆ˜ë¦½
def fit_kernel_svm(X,y):
    # ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” Kernel Functionì¸ Polynomial, RBF, Sigmoidì™€ ì¼ë°˜ Linear SVMì„ ì‹¤í—˜ ëŒ€ìƒìœ¼ë¡œ ì ìš©
    kernels = ['linear','poly','rbf','sigmoid'] 
    kernel_svms = []
    times = []

    for kernel in kernels:
        start_time = time.time() # ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ë˜ëŠ” ì‹œê°„ ì¸¡ì •ì„ ìœ„í•¨
        
        # 'linear_clf', 'poly_clf'ì™€ ê°™ì´ ê°ê¸° ë‹¤ë¥¸ ì»¤ë„ì„ ì‚¬ìš©í•œ SVC ëª¨ë¸ ê°ì²´ë¥¼ ë³€ìˆ˜ë¡œ ì €ì¥
        # ì•„ë˜ì™€ ê°™ì´ globals()['{}'.format()] í˜•íƒœë¥¼ ì´ìš©í•˜ë©´ ë™ì  ë³€ìˆ˜ ìƒì„± ê°€ëŠ¥
        globals()['{}_clf'.format(kernel)] = SVC(kernel = kernel).fit(X,y) 
        times.append(time.time()-start_time)
        kernel_svms.append(globals()['{}_clf'.format(kernel)]) # ê° Kernel Function ë³„ ìˆ˜ë¦½ëœ SVC ëª¨ë¸ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    
    return kernel_svms,times, kernels
```
```python
# Support Vector Classifier ì‹œê°í™” í•¨ìˆ˜
def svc_plot(X,y, svm_model):
    assert X.shape[1] == 2, "input X's Num of Feature should be 2" # 2ì°¨ì› ì‹œê°í™”ë¥¼ ìœ„í•´ Input Xì˜ ë³€ìˆ˜ ê°œìˆ˜ê°€ 2ê°œì¸ ê²½ìš°ë§Œ í—ˆìš©
    plt.title(f'{svm_model} Scatter Plot')
    plt.scatter(X[:,0],X[:,1], marker = 'o', c = y, cmap = plt.cm.Paired, edgecolors= 'k') # Data Instance ì‹œê°í™”

    # ì´ˆí‰ë©´ ì‹œê°í™”
    ax = plt.gca()    
    xlim, ylim = ax.get_xlim(), ax.get_ylim()
    xx = np.linspace(xlim[0],xlim[1], 10)
    yy = np.linspace(ylim[0],ylim[1], 10)
    XX, YY = np.meshgrid(xx, yy)

    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    z = svm_model.decision_function(xy).reshape(XX.shape) # ë¶„ë¥˜ ê²½ê³„ë©´ (Hyperplane)
    ax.contour(XX, YY, z, colors = ['k','r','k'], levels = [-1, 0, 1], alpha = 0.6, linestyles = ['--','-','--'])
    
    return plt.show()
```

```python
# ê° Kernel SVCì˜ ê²°ê³¼(ë°ì´í„° í”„ë ˆì„ ë° ê·¸ë˜í”„)ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
def show_kernel_svc_result(X, y):
    train_X, test_X, train_y, test_y = train_test_split(X, y, shuffle = True, test_size = 0.25)
    # ê° Kernel ë³„ SVC ëª¨ë¸ ìˆ˜ë¦½ + ê° Kernel ë³„ SVC ìˆ˜ë¦½ëœ ëª¨ë¸(kernel_svms)ê³¼ ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ëœ ì‹œê°„(times), Kernelë“¤ì˜ ëª…ì¹­(kernels) ì €ì¥
    kernel_svms, times, kernels = fit_kernel_svm(train_X, train_y) 
    k_svms, acc, precision, recall, f1 = [], [], [], [], [] # Kernel ë³„ ì„±ëŠ¥ì„ DataFrame í˜•íƒœë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±

    for i, k_s in enumerate(kernel_svms):
        k_svms.append(kernels[i]) # kernels[i] ì˜ˆì‹œ: 'linear' (str)
        acc.append(metric(k_s,test_X,test_y)[0]) # Classifierì˜ Accuracy 
        precision.append(metric(k_s,test_X,test_y)[1]) # Classifierì˜ Precision
        recall.append(metric(k_s,test_X,test_y)[2]) # Classifierì˜ Recall
        f1.append(metric(k_s,test_X,test_y)[3]) # Classifierì˜ F1 score

    # ê° ì»¤ë„ë³„ SVCì˜ ë¶„ë¥˜ ì„±ëŠ¥ ë° ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ëœ ì‹œê°„ì„ ë°ì´í„° í”„ë ˆì„ í˜•íƒœë¡œ ì €ì¥
    k_svm_result = pd.DataFrame({'Kernel':k_svms, 'Accuracy':acc, 'Precision':precision, 'Recall':recall, 'F1 Score':f1, 'Time for Train(s)': times})
    k_svm_result.iloc[:,1:] = k_svm_result.iloc[:,1:].apply(lambda x:np.round(x,4)) # Accuracy, Precision, Recall, F1-score, Time for Trainì„ ì†Œìˆ˜ì  ë„·ì§¸ìë¦¬ì—ì„œ ë°˜ì˜¬ë¦¼ (ê¼­ í•„ìš”í•œ ê³¼ì •ì€ ì•„ë‹˜)
    
    # Kernel ë³„ F1 Score ë° ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ ì‹œê°í™”
    fig, axes = plt.subplots(ncols = 2)
    fig.set_size_inches((8,5))
    fig.subplots_adjust(wspace = 0.3)

    axes[0].plot(k_svm_result['Kernel'], k_svm_result['F1 Score'], marker = 'o', color = 'blue')
    axes[0].set_title('F1 Score')
    axes[1].plot(k_svm_result['Kernel'], k_svm_result['Time for Train(s)'], marker = 'o', color = 'red')
    axes[1].set_title('Time for Train(s)')

    display(k_svm_result)
    plt.show()
```
- ìœ„ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì½”ë“œ ê²°ê³¼ ì˜ˆì‹œ
```python
# moon dataset ì ìš© ì˜ˆì‹œ
kernel_svms, times, kernels = fit_kernel_svm(X, y)

for k_s in kernel_svms:
    svc_plot(X,y,k_s)
```

<center class="half">
    <img src = "https://user-images.githubusercontent.com/56019094/199554240-0b8cd1e1-691e-46f0-bcfc-71ed7f774a9b.png" width = 400>
    <img src = "https://user-images.githubusercontent.com/56019094/199554255-46b85b76-916c-4516-b906-b992cfbefa2d.png" width = 400>
<figure>

<figure class="half">
    <img src = https://user-images.githubusercontent.com/56019094/199554282-46458a8e-21fd-474e-8e57-868cd0c560d6.png width = 400>
    <img src = https://user-images.githubusercontent.com/56019094/199554294-f908ba20-d081-48b3-90cf-0b981075ecc3.png width = 400>
</center>


### SVC ê²°ê³¼ ë° í•´ì„
```python
# Moon Dataset ì ìš© ê²°ê³¼ í™•ì¸
X, y = make_moons(n_samples = 5000, noise = 0.2, random_state = 1002)
show_kernel_svc_result(X,y)
```	
<center>

|Kernel|Accuracy|Precision|Recall|F1 Score|Time for Train(s)|
|------|--------|---------|------|--------|-----------------|
linear|	0.8728	| 0.8728| 	0.8728|	0.8728|	0.0556|
poly |	0.9008	| 0.9084| 	0.8998|	0.9002|	0.0584|
rbf	| 0.9752	| 0.9752| 	0.9752|	0.9752|	0.0204|
sigmoid	| 0.6184 |	0.6183| 0.6183|	0.6183|	0.0935|

</center>

<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199560207-277305dc-d1d7-4a7b-8151-3a084a76a2cf.png height = 300></p>

- ë…ë¦½ ë³€ìˆ˜ 2ê°œ, Data Instance ìˆ˜ 5000ê°œì¸ ìƒí™©ì—ì„œëŠ” ê¸°ì¡´ ì˜ˆìƒê³¼ ë‹¬ë¦¬ ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ëœ ì‹œê°„ì´ Sigmoid > Polynomial > Linear > RBF ìˆœìœ¼ë¡œ ê²°ê³¼ê°€ ë‚˜ì™”ìŒ  
- RBFì˜ ê²½ìš° ëª¨ë¸ ìˆ˜ë¦½ì— ê°€ì¥ ì ì€ ì‹œê°„ì´ ì†Œìš”ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  F1 Score ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥(ì•½ 0.98)ì„ ë³´ì´ë©° "Kernel SVMì„ ì ìš©í• ê±°ë¼ë©´ RBFë¥¼ ìš°ì„  ì ìš©í•´ë³´ë¼"ëŠ” ê¸°ì¡´ ê´€í–‰ì„ ë’·ë°›ì¹¨í•¨
- Polynomialì˜ ê²½ìš° ëª¨ë¸ ìˆ˜ë¦½ì— ê°€ì¥ ë§ì€ ì‹œê°„(RBFì˜ ë„¤ ë°° ì´ìƒ)ì´ ì†Œìš”ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  F1 Score ê¸°ì¤€ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ

```python
# Gaussian Quantiles ë°ì´í„°ì…‹ ì ìš© ê²°ê³¼ í™•ì¸
X, y = make_gaussian_quantiles(n_samples = 5000, n_features = 2, n_classes = 2, random_state = 42)
show_kernel_svc_result(X,y)
```
<center>

|Kernel|Accuracy|Precision|Recall|F1 Score|Time for Train(s)|
|------|--------|---------|------|--------|-----------------|
|linear|	0.5872|	0.6277|	0.5872|	0.5516|	0.1590|
|poly	|0.5144|	0.7537|	0.5144|	0.3646|	0.3005|
rbf|	0.9960	|0.9960|	0.9960|	0.9960|	0.0209|
sigmoid|	0.5328|	0.5328|	0.5328|	0.5328|	0.1886|

</center>

<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199562004-620c57e1-8129-4cf0-8820-07d4d5e55579.png height = 300></p>
- ë…ë¦½ ë³€ìˆ˜ 2ê°œ, Data Instance ìˆ˜ 5000ê°œì¸ ìƒí™©ì—ì„œ ì´ ë°ì´í„°ì…‹ ì—­ì‹œ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©°, make moon ë°ì´í„°ì…‹ê³¼ë„ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë³´ì˜€ìŒ
- Make Moon ë°ì´í„°ì…‹ì˜ ê²½ìš° [Sigmoid > Polynomial > Linear > RBF] ìˆœì„œì˜€ìœ¼ë‚˜ Gaussian Quantiles ë°ì´í„°ì…‹ì˜ ê²½ìš° [Polynomial > Sigmoid > Linear > RBF] ìˆœì„œ
- Polynomialì´ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦° ê²ƒì€ ì˜ˆìƒê³¼ ë™ì¼í•˜ë‚˜ RBFê°€ ê·¸ ë‹¤ìŒìœ¼ë¡œ ì˜¤ë˜ ê±¸ë¦´ ê²ƒì´ë¼ëŠ” ì˜ˆìƒê³¼ ë‹¬ë¦¬ Make Moon Datasetê³¼ Gaussian Quantiles ë°ì´í„°ì…‹ ì—­ì‹œ RBFëŠ” ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ ê°€ì¥ ì§§ì•˜ìœ¼ë©° F1 Score ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê¸°ê¹Œì§€ í•¨
- ì´ ê²½ìš°ì—ë„ Polynomialì€ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ ê°€ì¥ ì˜¤ë˜ ê±¸ë ¸ìœ¼ë‚˜ F1 Score ê¸°ì¤€ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ  

> ####  ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ì¦ê°€(1ê°œ -> 100ê°œ)ì‹œí‚¤ë©´ì„œ Kernel Function ë³„ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ ë³€í™” ì²´í¬
---
* í•´ë‹¹ ì‹¤í—˜ ì§„í–‰ ì´ìœ 
: Support Vector Regressor ì´ìš© ì‹¤í—˜ ì¤‘ "Kernel Function ìˆ˜ì‹ì„ ë³´ë‹ˆ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ê°€ ì¦ê°€í•˜ë©´ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ ì–´ë–»ê²Œ ë³€í• ê¹Œ"ë¼ëŠ” ì˜ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ [3, 6, 12, 24, 48]ë¡œ ëŠ˜ë¦¬ë©° ì‹¤í—˜ì„ í•´ë³´ë‹ˆ Kernel Function ë³„ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ ìˆœìœ„ê°€ ë³€ê²½ë˜ëŠ” í˜„ìƒì„ ë°œê²¬í•¨

<center>

ë…ë¦½ ë³€ìˆ˜ 3ê°œ
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199635510-dd217d8f-2c8b-4578-b02d-39e7b419f382.png height = 250 ></p>

ë…ë¦½ ë³€ìˆ˜ 6ê°œ
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199635778-db4773a0-4bef-4675-9d5c-0298216a63ed.png height = 250></p>

ë…ë¦½ ë³€ìˆ˜ 12ê°œ
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199635813-d348bd25-e594-464f-a6e0-3a812e9a9cda.png height = 250></p>

ë…ë¦½ ë³€ìˆ˜ 24ê°œ
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199635853-2ee21a27-afb0-4cbd-b80d-cf1122fe8822.png height = 250> </p>

ë…ë¦½ ë³€ìˆ˜ 48ê°œ
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199635894-597e423c-dadd-427d-a76e-009fc731915f.png height = 250></p>

</center>

---
- í•´ë‹¹ ì‹¤í—˜ì—ì„œ ì‚¬ìš©ëœ ì½”ë“œëŠ” ì´ì „ì— ì‚¬ìš©ë˜ì—ˆë˜ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•½ê°„ì˜ ë³€í˜•ë§Œ ì ìš©ë˜ì–´ì„œ Tutorial ë‚´ìš©ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ (ipynb íŒŒì¼ ì°¸ê³ )
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199563451-993a7962-a18c-4d33-8de9-bb21d2834e45.png></p>



- ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ 1ê°œì—ì„œ 100ê°œê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ì„œ, ê° Kernel Function ë³„ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ ë³„ë¡œ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ **ìµœì¥ ì‹œê°„**ì´ ê±¸ë ¸ë˜ ê²½ìš° Count

<center>

|Sigmoid|RBF|Polynomial|Linear|
|-------|---|----------|------|
|39|29|17|15|

</center>



- ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ 1ê°œì—ì„œ 100ê°œê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ì„œ, ê° Kernel Function ë³„ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ ë³„ë¡œ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ **ìµœë‹¨ ì‹œê°„**ì´ ê±¸ë ¸ë˜ ê²½ìš° Count

<center>

|Polynomial|Linear|RBF|Sigmoid|
|-------|---|----------|------|
|35|33|20|12|

</center>



- ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤ëŠ” ìƒˆë¡œìš´ ìƒí™©ì—ì„œ Kernel Function ë³„ ëª¨ë¸ í•™ìŠµì— ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì‹¤í—˜í•¨
- ìˆ˜ì‹ì— $degree$ ìŠ¹ì´ í¬í•¨ë˜ëŠ” Polynomialì˜ ê²½ìš° ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•  ê²ƒì´ë¼ ì˜ˆìƒí–ˆìœ¼ë‚˜ ë…ë¦½ ë³€ìˆ˜ê°€ 100ê°œê¹Œì§€ ì¦ê°€í•˜ëŠ” ìƒí™©ì—ì„œë„ ëª¨ë“  Kernel Functionì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŒ.
- ë„¤ ê°€ì§€ Kernel Functioì˜ ê²½ìš° ëª¨ë‘ ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì˜ ë¶„ì‚°ì´ ì»¤ì§€ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ë¨
- ë‚´ì  ì—°ì‚°ì´ ìˆ˜ì‹ì— í¬í•¨ë˜ëŠ” Polynomialê³¼ Sigmoidì˜ ê²½ìš° ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ ìœ ì‚¬í•  ê²ƒì´ë¼ ìƒê°í–ˆìœ¼ë‚˜, ê°ê° ê°€ì¥ ë§ì´ ìµœë‹¨ ì‹œê°„ì„ ê¸°ë¡í•œ ì»¤ë„ê³¼ ê°€ì¥ ë§ì´ ìµœì¥ ì‹œê°„ì„ ê¸°ë¡í•œ ì»¤ë„ì´ì—ˆìŒ
- Sigmoidì˜ ìˆ˜ì‹ì—ëŠ” $tanh$ê°€ í¬í•¨ë˜ê³ , Polynomialì˜ ê²½ìš° $degree$ìŠ¹ì´ í¬í•¨ë˜ì–´, ì´ ë‘˜ì„ ìˆ˜ì‹ë§Œ ë³´ì•˜ì„ ë•Œì—ëŠ” Polynomialì´ ë” ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì´ë¼ íŒë‹¨í–ˆìœ¼ë‚˜ ìƒë°˜ëœ ê²°ê³¼ê°€ ë‚˜ì™€ì„œ í¥ë¯¸ë¡œì› ìŒ. ì´ëŸ¬í•œ ê²°ê³¼ê°€ ë‚˜ì˜¨ ì´ìœ ì— ëŒ€í•´ì„œëŠ” ë°ì´í„°ì…‹ ë³€ê²½ì„ í†µí•œ ì¶”ê°€ ì‹¤í—˜ ë° ì‹œê°„ ë³µì¡ë„ì— ëŒ€í•œ ê°œë…ì„ ì¶”ê°€ì ìœ¼ë¡œ ê³µë¶€í•´ì•¼ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ íŒë‹¨ë¨.

## *Additional Experiment - Support Vector Regressor*
- í•´ë‹¹ ì‹¤í—˜ì—ì„œ ì‚¬ìš©ëœ ì½”ë“œëŠ” ì´ì „ì— ì‚¬ìš©ë˜ì—ˆë˜ SVC ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•½ê°„ì˜ ë³€í˜•(SVC -> SVR ë“±)ë§Œ ì ìš©ë˜ì–´ì„œ Tutorial ë‚´ìš©ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ (ipynb íŒŒì¼ ì°¸ê³ )
- ë°ì´í„°ì…‹ ìƒì„±ì—ëŠ” ì‚¬ì´í‚·ëŸ°ì˜ make_regression ë©”ì†Œë“œë¥¼ ì‚¬ìš©í–ˆìŒ (n_featuresë¥¼ í†µí•´ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ììœ ë¡­ê²Œ ì¡°ì • ê°€ëŠ¥í•¨)
- SVCì˜ ë§ˆì§€ë§‰ ì‹¤í—˜ê³¼ ë™ì¼í•˜ê²Œ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ 1ê°œë¶€í„° 100ê°œê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ì„œ Kernel Function(Linear, Polynomial, RBF, Sigmoid) ë³„ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ ë³€í™” ì‹¤í—˜

### SVR ê²°ê³¼ ë° í•´ì„
<p align = 'center'><img src = https://user-images.githubusercontent.com/56019094/199567437-870bbdf6-564a-485b-a6b0-6ec7d33a6e98.png></p>

- ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ 1ê°œì—ì„œ 100ê°œê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ì„œ, ê° Kernel Function ë³„ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ ë³„ë¡œ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ **ìµœì¥ ì‹œê°„**ì´ ê±¸ë ¸ë˜ ê²½ìš° Count

<center>

|Sigmoid|Linear|Polynomial|RBF|
|-------|---|----------|------|
|34|31|19|16|

</center>


- ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ 1ê°œì—ì„œ 100ê°œê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ì„œ, ê° Kernel Function ë³„ ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ ë³„ë¡œ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì´ **ìµœë‹¨ ì‹œê°„**ì´ ê±¸ë ¸ë˜ ê²½ìš° Count


<center>

|Polynomial|RBF|Linear|Sigmoid|
|-------|---|----------|------|
|32|27|23|18|

</center>

- ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ê° Kernel Function ë³„ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì˜ ë¶„ì‚°ì´ ì»¤ì§€ëŠ” ê²ƒì€ SVCì™€ ë™ì¼í•˜ë‚˜ ë¶„ì‚°ì˜ í¬ê¸°ê°€ ë” í¼
- ì²« ë²ˆì§¸ í‘œë¥¼ ë³´ë©´, SVCì˜ ê²°ê³¼ì™€ ë™ì¼í•˜ê²Œ Sigmoidê°€ ê°€ì¥ ë§ì´ ìµœì¥ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì„ ê¸°ë¡í–ˆìŒ
- ë‘ ë²ˆì§¸ í‘œë¥¼ ë³´ë©´, SVCì˜ ê²°ê³¼ì™€ ë™ì¼í•˜ê²Œ Polynomialì´ ê°€ì¥ ë§ì´ ìµœë‹¨ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì„ ê¸°ë¡í–ˆìŒ

## ê²°ë¡ 
- ì‹¤í—˜ ì „ ì˜ˆìƒ: [Linear < Sigmoid < RBF < Polynomial] ìˆœìœ¼ë¡œ ëª¨ë¸ ìˆ˜ë¦½ì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ë  ê²ƒ
- SVC ê²°ê³¼(ìµœë‹¨ ì‹œê°„ ê²°ê³¼ í‘œ ê¸°ë°˜): [Polynomial < Linear < RBF< Sigmoid]
- SVR ê²°ê³¼(ìµœë‹¨ ì‹œê°„ ê²°ê³¼ í‘œ ê¸°ë°˜): [Polynomai < RBF < Linear < Sigmoid]
- ê¸°ì¡´ ì˜ˆìƒê³¼ ë°˜ëŒ€ë¡œ Polynomialì´ ëª¨ë¸ í•™ìŠµì— ê°€ì¥ ì ì€ ì‹œê°„ì´ ì†Œìš”ë˜ì—ˆìœ¼ë©° Sigmoidê°€ ê°€ì¥ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ì—ˆìŒ
- ê·¸ëŸ¬ë‚˜ ëª¨ë¸ ì„±ëŠ¥ì„ ê³ ë ¤í–ˆì„ ë•Œì—ëŠ” SVCì™€ SVR ëª¨ë‘ RBFê°€ ëŒ€ë¶€ë¶„ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ (SVRì˜ ê²½ìš° í•´ë‹¹ ì‹¤í—˜ íŒŒíŠ¸ëŠ” ì½”ë“œì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŒ)
- Polynomial Kernel SVMì´ ëª¨ë¸ í•™ìŠµ ì†Œìš” ì‹œê°„ ì¸¡ë©´ì—ì„œëŠ” ì¥ì ì´ ìˆìœ¼ë‚˜ ì„±ëŠ¥ì„ ê³ ë ¤í–ˆì„ ë•Œì—ëŠ” ë‹¤ë¥¸ Kernel Functionì— ë¹„í•´ ì„±ëŠ¥ì´ ë‚®ìŒ
- ì—¬ëŸ¬ ê°€ì§€ Kernel Functionì„ ìˆ˜ë¦½í•´ë³´ê¸°ì— ì‹œê°„ì´ ë¶€ì¡±í•œ ê²½ìš°ì—ëŠ” RBF Kernel SVMì„ ìš°ì„ ì ìœ¼ë¡œ ì‹œë„í•´ë³´ëŠ” ê²ƒì´ í•©ë¦¬ì  (ì‚¬ì´í‚·ëŸ° SVMì˜ Kernel Defaultê°€ RBFì¸ ì´ìœ ë„ ì´ëŸ¬í•œ ë°°ê²½ì´ ìˆì—ˆì„ ê²ƒì´ë¼ ìœ ì¶”ë¨)
- ìµœì¢… ê²°ë¡ : **Kernel ë³„ë¡œ SVM ëª¨ë¸ ìˆ˜ë¦½ì— ì†Œìš”ë˜ëŠ” ì‹œê°„** ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨. Polynomialì˜ ê°€ì¥ ì ì€ ì‹œê°„ì´ ì†Œìš”ë˜ì—ˆê³ , Sigmoidê°€ ê°€ì¥ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ì—ˆìŒ

- í•œê³„ì 
    - SVCì˜ ê²½ìš° ë‘ ê°€ì§€ ë°ì´í„°ì…‹(ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ ëª¨ë¸ ìˆ˜ë¦½ ì†Œìš” ì‹œê°„ì„ í™•ì¸í•œ ì‹¤í—˜ì—ì„œëŠ” í•œ ê°€ì§€ ë°ì´í„°ì…‹ë§Œ ì‚¬ìš©), SVRì˜ ê²½ìš° í•œ ê°€ì§€ ë°ì´í„°ì…‹ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜í•¨
    - Kernel Function ë³„ ìˆ˜ì‹ê³¼ ì‹¤í—˜ ê²°ê³¼ì™€ì˜ ì—°ê´€ì„±ì„ ì •í™•í•˜ê²Œ í•´ì„í•´ë‚´ì§€ ëª»í–ˆìŒ
